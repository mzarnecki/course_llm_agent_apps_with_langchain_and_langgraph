{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4dc8e4a",
   "metadata": {},
   "source": [
    "# Model-agnostic Gateway + Fallback + Feature Flags + Koszty (realny przyklad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d11815a",
   "metadata": {},
   "source": [
    "\n",
    "Ten notebook pokazuje produkcyjny (ale uproszczony) wzorzec:\n",
    "- Model-agnostic gateway — wybor modelu po naglowku `X-Model` i/lub przez feature flags.\n",
    "- Fallback — timeout lub blad -> automatyczne przelaczenie na model zapasowy (np. `oss:mock`).\n",
    "- Feature flags (canary / versioning) — procentowe kierowanie czesci ruchu na nowy model / prompt.\n",
    "- Monitoring kosztow i metryk — mierzymy latency, liczymy koszty per zapytanie / dziennie, progi alarmowe.\n",
    "\n",
    "Notebook nie wymaga zewnetrznych kluczy API — uzywamy lekkich mockow modeli.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b209d68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T06:05:59.573589Z",
     "start_time": "2025-10-24T06:05:59.255928Z"
    }
   },
   "source": [
    "# === Imports & helpers ===\n",
    "from __future__ import annotations\n",
    "from fastapi import FastAPI, Header\n",
    "from fastapi.testclient import TestClient\n",
    "from pydantic import BaseModel\n",
    "from datetime import datetime, date\n",
    "import time, random, hashlib\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "# Prosty zegar (ms)\n",
    "def now_ms():\n",
    "    return int(time.time() * 1000)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "3d0bbc21",
   "metadata": {},
   "source": [
    "## Konfiguracja: feature flags i stawki kosztow"
   ]
  },
  {
   "cell_type": "code",
   "id": "21e18f4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T06:05:59.601082Z",
     "start_time": "2025-10-24T06:05:59.597570Z"
    }
   },
   "source": [
    "# === Feature flags ===\n",
    "# - canary_rollout: % ruchu kierowany do nowego modelu\n",
    "# - prompt_versions: szybkie przelaczanie promptow\n",
    "\n",
    "FEATURE_FLAGS = {\n",
    "    'canary_rollout_percent': 15,\n",
    "    'canary_model': 'openai:gpt-4o-mini-canary',\n",
    "    'stable_model': 'openai:gpt-4o-mini',\n",
    "    'prompt_versions': {\n",
    "        'v1': 'You are a concise assistant. Answer briefly.',\n",
    "        'v2': 'You are a structured assistant. Answer in bullet points with 2-4 bullets.'\n",
    "    },\n",
    "    'prompt_default': 'v1',\n",
    "}\n",
    "\n",
    "# === Stawki kosztow (mock) ===\n",
    "COST_RATES = {\n",
    "    'openai:gpt-4o-mini': 0.0000025,\n",
    "    'openai:gpt-4o-mini-canary': 0.0000030,\n",
    "    'anthropic:sonnet': 0.0000032,\n",
    "    'oss:mock': 0.0,\n",
    "}\n",
    "\n",
    "DAILY_ALERT_THRESHOLD_USD = 50.0\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "0f126c07",
   "metadata": {},
   "source": [
    "## Monitoring: koszty, latency, progi alarmowe"
   ]
  },
  {
   "cell_type": "code",
   "id": "3f351c10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T06:05:59.656144Z",
     "start_time": "2025-10-24T06:05:59.649245Z"
    }
   },
   "source": [
    "METRICS = {\n",
    "    'requests': [],\n",
    "    'daily_costs': {},\n",
    "}\n",
    "\n",
    "def _hash_to_percent(s: str) -> int:\n",
    "    h = hashlib.sha256(s.encode('utf-8')).hexdigest()\n",
    "    return int(h[:8], 16) % 100\n",
    "\n",
    "def pick_model_for_user(user_id: str) -> str:\n",
    "    # wybor wg canary rollout\n",
    "    bucket = _hash_to_percent(user_id or 'anonymous')\n",
    "    if bucket < int(FEATURE_FLAGS['canary_rollout_percent']):\n",
    "        return FEATURE_FLAGS['canary_model']\n",
    "    return FEATURE_FLAGS['stable_model']\n",
    "\n",
    "def pick_prompt_version(user_id: str, override: Optional[str] = None) -> str:\n",
    "    if override and override in FEATURE_FLAGS['prompt_versions']:\n",
    "        return override\n",
    "    return FEATURE_FLAGS['prompt_default']\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    return max(1, len(text) // 4)\n",
    "\n",
    "def compute_cost(model_name: str, prompt: str, response: str) -> float:\n",
    "    rate = COST_RATES.get(model_name, 0.000003)\n",
    "    tokens = estimate_tokens(prompt) + estimate_tokens(response)\n",
    "    return round(tokens * rate, 6)\n",
    "\n",
    "def log_request(user_id: str, model: str, latency_ms: int, cost_usd: float, ok: bool, prompt_ver: str):\n",
    "    METRICS['requests'].append((now_ms(), user_id, model, latency_ms, cost_usd, ok, prompt_ver))\n",
    "    day = date.today().isoformat()\n",
    "    METRICS['daily_costs'][day] = METRICS['daily_costs'].get(day, 0.0) + cost_usd\n",
    "    if METRICS['daily_costs'][day] > DAILY_ALERT_THRESHOLD_USD:\n",
    "        print('ALERT: Daily cost exceeded', DAILY_ALERT_THRESHOLD_USD, 'USD! Current:', round(METRICS['daily_costs'][day], 2))\n",
    "\n",
    "def get_daily_cost(day: Optional[str] = None) -> float:\n",
    "    return METRICS['daily_costs'].get(day or date.today().isoformat(), 0.0)\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "eb49457f",
   "metadata": {},
   "source": [
    "## Mocki modeli (OpenAI / Anthropic / OSS) z kontrola timeoutu"
   ]
  },
  {
   "cell_type": "code",
   "id": "05cff9dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T06:05:59.706650Z",
     "start_time": "2025-10-24T06:05:59.702690Z"
    }
   },
   "source": [
    "class LLMProvider:\n",
    "    def __init__(self, name: str, mean_latency_ms: int = 120, p_timeout: float = 0.02):\n",
    "        self.name = name\n",
    "        self.mean_latency_ms = mean_latency_ms\n",
    "        self.p_timeout = p_timeout\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        # latency: exp around mean\n",
    "        delay = random.expovariate(1.0 / max(1, self.mean_latency_ms))\n",
    "        if random.random() < 0.05:\n",
    "            delay *= 3.5  # long tail\n",
    "        time.sleep(delay / 1000.0)\n",
    "        if random.random() < self.p_timeout:\n",
    "            raise TimeoutError(f'Model {self.name} timeout')\n",
    "        return f'[{self.name}] Answer to: ' + prompt[:120]\n",
    "\n",
    "OPENAI_MINI = LLMProvider('openai:gpt-4o-mini', mean_latency_ms=140, p_timeout=0.04)\n",
    "OPENAI_CANARY = LLMProvider('openai:gpt-4o-mini-canary', mean_latency_ms=160, p_timeout=0.05)\n",
    "ANTHROPIC_SONNET = LLMProvider('anthropic:sonnet', mean_latency_ms=170, p_timeout=0.03)\n",
    "\n",
    "class OSSMock:\n",
    "    def __init__(self):\n",
    "        self.name = 'oss:mock'\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        time.sleep(0.02)\n",
    "        return f'(OSS fallback) Echo: {prompt[:160]}'\n",
    "\n",
    "OSS = OSSMock()\n",
    "\n",
    "MODEL_REGISTRY: Dict[str, Any] = {\n",
    "    'openai:gpt-4o-mini': OPENAI_MINI,\n",
    "    'openai:gpt-4o-mini-canary': OPENAI_CANARY,\n",
    "    'anthropic:sonnet': ANTHROPIC_SONNET,\n",
    "    'oss:mock': OSS,\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "9e77b06e",
   "metadata": {},
   "source": [
    "## FastAPI: endpoint `/chat` z naglowkami `X-Model`, `X-Prompt-Version`, `X-User-Id`"
   ]
  },
  {
   "cell_type": "code",
   "id": "a0f675f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T06:05:59.760075Z",
     "start_time": "2025-10-24T06:05:59.752284Z"
    }
   },
   "source": [
    "app = FastAPI(title='Model-agnostic Gateway (demo)')\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "\n",
    "def build_prompt(base_system: str, user_message: str) -> str:\n",
    "    return base_system + '\\nUser: ' + user_message\n",
    "\n",
    "def call_with_fallback(model_name: str, prompt: str, timeout_ms: int = 2000) -> Tuple[str, str, bool, int]:\n",
    "    # Try primary; on error -> fallback to oss:mock. Returns (response, final_model, ok, latency_ms)\n",
    "    start = now_ms()\n",
    "    try:\n",
    "        prov = MODEL_REGISTRY[model_name]\n",
    "        resp = prov.invoke(prompt)\n",
    "        latency = now_ms() - start\n",
    "        return resp, model_name, True, latency\n",
    "    except Exception:\n",
    "        resp = MODEL_REGISTRY['oss:mock'].invoke(prompt)\n",
    "        latency = now_ms() - start\n",
    "        return resp, 'oss:mock', False, latency\n",
    "\n",
    "@app.post('/chat')\n",
    "def chat_endpoint(\n",
    "    req: ChatRequest,\n",
    "    x_model: Optional[str] = Header(default=None, alias='X-Model'),\n",
    "    x_prompt_version: Optional[str] = Header(default=None, alias='X-Prompt-Version'),\n",
    "    x_user_id: Optional[str] = Header(default='anonymous', alias='X-User-Id'),\n",
    "):\n",
    "    # 1) model selection\n",
    "    if x_model:\n",
    "        if x_model not in MODEL_REGISTRY:\n",
    "            return {'error': f'Unknown model: {x_model}'}\n",
    "        model_name = x_model\n",
    "    else:\n",
    "        model_name = pick_model_for_user(x_user_id or 'anonymous')\n",
    "\n",
    "    # 2) prompt version\n",
    "    prompt_ver = pick_prompt_version(x_user_id or 'anonymous', override=x_prompt_version)\n",
    "    system_prompt = FEATURE_FLAGS['prompt_versions'][prompt_ver]\n",
    "\n",
    "    # 3) final prompt\n",
    "    final_prompt = build_prompt(system_prompt, req.message)\n",
    "\n",
    "    # 4) call with fallback\n",
    "    response_text, final_model, ok, latency_ms = call_with_fallback(model_name, final_prompt)\n",
    "\n",
    "    # 5) cost & metrics\n",
    "    cost = compute_cost(final_model, final_prompt, response_text)\n",
    "    log_request(user_id=x_user_id or 'anonymous', model=final_model,\n",
    "                latency_ms=latency_ms, cost_usd=cost, ok=ok, prompt_ver=prompt_ver)\n",
    "\n",
    "    return {\n",
    "        'model': final_model,\n",
    "        'ok': ok,\n",
    "        'latency_ms': latency_ms,\n",
    "        'cost_usd': cost,\n",
    "        'prompt_version': prompt_ver,\n",
    "        'answer': response_text,\n",
    "        'daily_cost_usd': round(get_daily_cost(), 4),\n",
    "    }\n",
    "\n",
    "client = TestClient(app)\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "53c8ae9d",
   "metadata": {},
   "source": [
    "## Demo: wywolania z roznymi naglowkami (model, prompt, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "id": "fc19a604",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T06:06:00.705326Z",
     "start_time": "2025-10-24T06:05:59.806688Z"
    }
   },
   "source": [
    "def demo_call(message: str, user_id: str, model: Optional[str] = None, pver: Optional[str] = None):\n",
    "    headers = {'X-User-Id': user_id}\n",
    "    if model:\n",
    "        headers['X-Model'] = model\n",
    "    if pver:\n",
    "        headers['X-Prompt-Version'] = pver\n",
    "    r = client.post('/chat', json={'message': message}, headers=headers)\n",
    "    return r.json()\n",
    "\n",
    "# 1) Bez X-Model -> routing wg canary (czesc userow na canary)\n",
    "out1 = demo_call('Give me 3 key benefits of a model-agnostic gateway.', user_id='alice')\n",
    "out2 = demo_call('Give me 3 key benefits of a model-agnostic gateway.', user_id='bob')\n",
    "out3 = demo_call('Give me 3 key benefits of a model-agnostic gateway.', user_id='carol')\n",
    "\n",
    "# 2) Wymuszenie modelu OpenAI stable + prompt v2\n",
    "out4 = demo_call('Short plan for adding fallback to my API.', user_id='dave', model='openai:gpt-4o-mini', pver='v2')\n",
    "\n",
    "# 3) Wymuszenie nieznanego modelu -> blad\n",
    "bad = client.post('/chat', json={'message': 'test'}, headers={'X-Model': 'unknown:model'}).json()\n",
    "\n",
    "# 4) Wymuszenie Anthropic (mock) - inny koszt\n",
    "out5 = demo_call('List 2 risks of vendor lock-in.', user_id='erin', model='anthropic:sonnet')\n",
    "\n",
    "print('1) alice ->', out1['model'], 'cost:', out1['cost_usd'], 'ok:', out1['ok'])\n",
    "print('2) bob   ->', out2['model'], 'cost:', out2['cost_usd'], 'ok:', out2['ok'])\n",
    "print('3) carol ->', out3['model'], 'cost:', out3['cost_usd'], 'ok:', out3['ok'])\n",
    "print('4) dave  ->', out4['model'], 'cost:', out4['cost_usd'], 'ok:', out4['ok'], 'prompt:', out4['prompt_version'])\n",
    "print('5) erin  ->', out5['model'], 'cost:', out5['cost_usd'], 'ok:', out5['ok'])\n",
    "print('bad model ->', bad)\n",
    "print('Dzisiejszy koszt (USD):', round(get_daily_cost(), 4))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) alice -> openai:gpt-4o-mini-canary cost: 0.00018 ok: True\n",
      "2) bob   -> openai:gpt-4o-mini-canary cost: 0.00018 ok: True\n",
      "3) carol -> openai:gpt-4o-mini cost: 0.000145 ok: True\n",
      "4) dave  -> openai:gpt-4o-mini cost: 0.00017 ok: True prompt: v2\n",
      "5) erin  -> anthropic:sonnet cost: 0.000154 ok: True\n",
      "bad model -> {'error': 'Unknown model: unknown:model'}\n",
      "Dzisiejszy koszt (USD): 0.0008\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "79510777",
   "metadata": {},
   "source": [
    "## Demo progu kosztow (alert)"
   ]
  },
  {
   "cell_type": "code",
   "id": "4aaf128e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T06:06:08.438921Z",
     "start_time": "2025-10-24T06:06:02.934367Z"
    }
   },
   "source": [
    "for i in range(30):\n",
    "    _ = demo_call(f'Run #{i}: summarize gateway pattern.', user_id=f'user{i%3}')\n",
    "print('Dzisiejszy koszt po petli:', round(get_daily_cost(), 4), 'USD')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dzisiejszy koszt po petli: 0.0043 USD\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "d0090524",
   "metadata": {},
   "source": [
    "## Metryki: podglad ostatnich 5 prosb"
   ]
  },
  {
   "cell_type": "code",
   "id": "4ab60342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T06:06:08.449670Z",
     "start_time": "2025-10-24T06:06:08.446262Z"
    }
   },
   "source": [
    "from collections import deque\n",
    "tail = deque(METRICS['requests'][-5:], maxlen=5)\n",
    "for ts, uid, model, lat, cost, ok, pver in tail:\n",
    "    ts_iso = datetime.fromtimestamp(ts/1000).isoformat(timespec='seconds')\n",
    "    print(f\"{ts_iso} | user={uid:8s} | model={model:24s} | latency={lat:4d}ms | cost=${cost:.6f} | ok={ok} | prompt={pver}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-24T08:06:07 | user=user1    | model=openai:gpt-4o-mini       | latency=  65ms | cost=$0.000125 | ok=True | prompt=v1\n",
      "2025-10-24T08:06:07 | user=user2    | model=openai:gpt-4o-mini       | latency= 156ms | cost=$0.000125 | ok=True | prompt=v1\n",
      "2025-10-24T08:06:07 | user=user0    | model=openai:gpt-4o-mini       | latency= 100ms | cost=$0.000125 | ok=True | prompt=v1\n",
      "2025-10-24T08:06:08 | user=user1    | model=openai:gpt-4o-mini       | latency= 516ms | cost=$0.000125 | ok=True | prompt=v1\n",
      "2025-10-24T08:06:08 | user=user2    | model=openai:gpt-4o-mini       | latency= 137ms | cost=$0.000125 | ok=True | prompt=v1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "d1bda449",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Tekst do odcinka (skrót)\n",
    "\n",
    "W tym odcinku przechodzimy z etapu eksperymentow do wdrozen produkcyjnych... Dodaj tu swoj pelny skrypt do telepromptera."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
