{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2. Komponenty biblioteki LangChain",
   "id": "be304df0c16cc932"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Instalacja bibliotek",
   "id": "a4b1b26363bfed05"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-16T06:01:58.417365Z",
     "start_time": "2025-09-16T06:01:56.953704Z"
    }
   },
   "source": "!pip install -q python-dotenv langchain langchain-openai langchain-community langchain-text-splitters faiss-cpu",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Konfiguracja .env i modelu",
   "id": "20734714333be78c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T06:04:48.293085Z",
     "start_time": "2025-09-16T06:04:46.307679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Jesteś pomocnym asystentem. Odpowiadaj zwięźle.\"),\n",
    "    (\"user\", \"Streśc w 1 zdaniu: {tekst}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()  # LCEL: prompt → model → parser\n",
    "result = chain.invoke({\"tekst\": \"LangChain to biblioteka, która powstała w 2022 roku, w momencie kiedy świat zaczął odkrywać praktyczne możliwości dużych modeli językowych takich jak GPT-3. Jej twórcy zauważyli, że większość projektów opartych na LLM powtarza podobne wzorce – na przykład łączenie promptów w sekwencje, przechowywanie kontekstu rozmowy, wykonywanie zapytań do baz danych oraz wyszukiwarek. Zamiast budować to wszystko od zera, stworzyli bibliotekę, która dostarcza gotowe klocki do składania aplikacji. Dzięki LangChain możemy szybko tworzyć prototypy, a później rozwijać je w kierunku produkcyjnym. Biblioteka stała się swoistym standardem w świecie deweloperów AI, bo pozwala łatwo integrować modele różnych dostawców – nie tylko OpenAI, ale też Anthropic, HuggingFace czy lokalne modele open-source.\"})\n",
    "print(result)\n"
   ],
   "id": "fd2b0cdbcb177adb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain to biblioteka stworzona w 2022 roku, która ułatwia tworzenie aplikacji opartych na dużych modelach językowych, oferując gotowe komponenty do integracji i prototypowania, co czyni ją standardem wśród deweloperów AI.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prompt template\n",
    "ChatPromptTemplate w LangChain to szablon do budowania ustrukturyzowanych promptów dla modeli czatu.\n",
    "Pozwala definiować role (system, user, assistant), zmienne i format wiadomości, aby w spójny i wielokrotnego użytku sposób generować wejścia dla LLM."
   ],
   "id": "9f0f36cc0a222968"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T06:07:48.347701Z",
     "start_time": "2025-09-16T06:07:46.901382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "template = \"\"\"\n",
    "Specify tags and the main topic of the text.\n",
    "tags: What are best tags describing text. Give maximum 5 tags separated by comma.\n",
    "topic: What is the topic of text. Use maximum couple of words\n",
    "\n",
    "Format response as JSON as below. Return only JSON without markdown tags.\n",
    "'tags': ['sometag', 'othertag', 'anothertag', 'tag4', 'tag5']\n",
    "'subject': 'Some subject of text'\n",
    "\n",
    "text: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=template)\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "chain.invoke({\"input\":\"They picked a way among the trees, and their ponies plodded along, carefully avoiding the many writhing and interlacing roots.  There was no undergrowth.  The ground was rising steadily, and as they went forward it seemed that the trees became taller, darker, and thicker. There was no sound, except an occasional drip of moisture falling through the still leaves.  For the moment there was no whispering or movement among the branches; but they all got an uncomfortable feeling that they were being watched with disapproval, deepening to dislike and even enmity.  The feeling steadily grew, until they found themselves looking up quickly, or glancing back over their shoulders, as if they expected a sudden blow.\"})"
   ],
   "id": "58d640276960a0c0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{\\n    'tags': ['forest', 'suspense', 'nature', 'mystery', 'journey'],\\n    'subject': 'Forest Exploration'\\n}\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ResponseSchema i OutputParser\n",
    "ResponseSchema w LangChain to definicja oczekiwanego formatu odpowiedzi (np. pola JSON z nazwą, typem, opisem), która pomaga modelowi zwracać dane w przewidywalnej strukturze.\n",
    "\n",
    "OutputParser interpretuje surową odpowiedź LLM i przekształca ją w żądaną formę (np. obiekt Pydantic, JSON, lista), ułatwiając dalsze użycie w aplikacji."
   ],
   "id": "c7e817465b616462"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T06:15:57.624419Z",
     "start_time": "2025-09-16T06:15:56.230611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "tags_schema = ResponseSchema(\n",
    "    name=\"tags\",\n",
    "    description=\" What are best tags describing text. Give maximum 5 tags separated by comma.\",\n",
    ")\n",
    "topic_schema = ResponseSchema(\n",
    "    name=\"topic\",\n",
    "    description=\"What is the topic of text. Use maximum couple of words.\"\n",
    ")\n",
    "\n",
    "response_schemas = [tags_schema, topic_schema]\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "template = \"\"\"\n",
    "Specify tags and the main topic of the text. Return only JSON without markdown tags.\n",
    "tags: What are best tags describing text. Give maximum 5 tags separated by comma.\n",
    "topic: What is the topic of text. Use maximum couple of words\n",
    "\n",
    "Format response as JSON as below:\n",
    "'tags': ['sometag', 'othertag', 'anothertag', 'tag4', 'tag5']\n",
    "'subject': 'Some subject of text'\n",
    "\n",
    "text: {input}\n",
    "\n",
    "{instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "instructions = parser.get_format_instructions()\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "chain = prompt | llm | parser\n",
    "output_dict = chain.invoke({\n",
    "    \"input\":\"They picked a way among the trees, and their ponies plodded along, carefully avoiding the many writhing and interlacing roots.  There was no undergrowth.  The ground was rising steadily, and as they went forward it seemed that the trees became taller, darker, and thicker. There was no sound, except an occasional drip of moisture falling through the still leaves.  For the moment there was no whispering or movement among the branches; but they all got an uncomfortable feeling that they were being watched with disapproval, deepening to dislike and even enmity.  The feeling steadily grew, until they found themselves looking up quickly, or glancing back over their shoulders, as if they expected a sudden blow.\",\n",
    "    \"instructions\":instructions})\n",
    "print(output_dict)"
   ],
   "id": "765ef2a9b758f243",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['fantasy', 'adventure', 'mystery', 'forest', 'suspense'], 'topic': 'Journey through the forest'}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Memory (historia rozmowy / stan)",
   "id": "c928b1583cceade5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "74259c34319be37c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Prowadzisz przyjazną rozmowę i pamiętasz kontekst.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chat_chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "store = {}  # prosta “baza” historii po session_id\n",
    "\n",
    "def get_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chat_chain,\n",
    "    get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "sid = \"demo-session-1\"\n",
    "print(chain_with_memory.invoke({\"input\": \"Cześć! Mam na imię Michał.\"},\n",
    "                               config={\"configurable\": {\"session_id\": sid}}))\n",
    "print(chain_with_memory.invoke({\"input\": \"Jak mam na imię?\"},\n",
    "                               config={\"configurable\": {\"session_id\": sid}}))\n"
   ],
   "id": "9762f0fd06e70f4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Memory",
   "id": "bf1a30db19a6c9ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# memory\n",
    "# History keeps all messages between the user and AI intact. History is what the user sees in the UI.\n",
    "# It represents what was actually said. Memory keeps some information, which is presented to the LLM to make it behave as if it \"remembers\" the conversation.\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"Buenos dias!\")\n",
    "memory.chat_memory.add_ai_message(\"Hello!\")\n",
    "memory.chat_memory.add_user_message(\"Whats your name?\")\n",
    "memory.chat_memory.add_ai_message(\"My name is GIGACHAT\")\n",
    "memory.load_memory_variables({})"
   ],
   "id": "51e3819f14eb5b3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=True, memory=memory\n",
    ")\n",
    "conversation.predict(input=\"Buenos Dias!\") # will it response in different language?"
   ],
   "id": "d255837802009235"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Summary",
   "id": "91ce1fe6c1c562a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install tiktoken",
   "id": "f68f37f98302d2cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "previous_output_review = \"Rdr2 is an experience, as it's more than just a video game. It's like being in one long Oscar winning movie. Script, acting, storyline are immense, it's quite unbelievable really. You are living as a cowboy day in day out eating, bathing, sleeping, everything in a huge openworld.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=1-0)\n",
    "memory.save_context(\n",
    "    {\"input\": f\"Could you analyze review for me {previous_output_review}?\"},\n",
    "    {\"output\": \"Sure, no problem\"},\n",
    ")"
   ],
   "id": "506466fbdd6c0e60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)",
   "id": "921a0c2fb84b36c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "conversation.predict(input=\"Thank you\")",
   "id": "3c53b0478c8f324c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
