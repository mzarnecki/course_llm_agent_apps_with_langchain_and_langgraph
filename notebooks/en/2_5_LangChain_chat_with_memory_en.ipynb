{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa55855baa5d0aa5",
   "metadata": {},
   "source": "## Chat with memory"
  },
  {
   "cell_type": "markdown",
   "id": "6d16ab59db57af29",
   "metadata": {},
   "source": "### Install libraries"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-04T06:46:25.883150Z",
     "start_time": "2025-11-04T06:46:24.399860Z"
    }
   },
   "source": [
    "!pip install -q langchain-openai python-dotenv"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "d8fdb3af72d3a768",
   "metadata": {},
   "source": "### Load env variables"
  },
  {
   "cell_type": "code",
   "id": "eb55511e53f3a772",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:46:25.901209Z",
     "start_time": "2025-11-04T06:46:25.895141Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "155809f08030346c",
   "metadata": {},
   "source": [
    "### History\n",
    "History – stores and injects the entire conversation history message by message into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "id": "8634db480b86ed6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:46:25.959065Z",
     "start_time": "2025-11-04T06:46:25.956022Z"
    }
   },
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "# message history\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"Buenos dias!\")\n",
    "history.add_ai_message(\"hello!\")\n",
    "history.add_user_message(\"Whats your name?\")\n",
    "history.add_ai_message(\"My name is GIGACHAT\")\n",
    "\n",
    "# history.messages"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "1dc59fc6574e8ed9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:46:26.023887Z",
     "start_time": "2025-11-04T06:46:26.019220Z"
    }
   },
   "source": [
    "history.messages"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buenos dias!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hello!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Whats your name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='My name is GIGACHAT', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "b66acacd555b1e9c",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec4e0329412c0a52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:46:29.842028Z",
     "start_time": "2025-11-04T06:46:26.124327Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# message types: user, assistant, system, function, tool\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You have a friendly conversation and remember the context.\"\n",
    "               \"Respond in english.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "store = {}\n",
    "def get_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "sid = \"demo-session-123\"\n",
    "\n",
    "resp1 = chain_with_memory.invoke(\n",
    "    {\"input\": \"Hello. My name is Walter White but everyone call me Heisenberg.\"},\n",
    "    config={\"configurable\": {\"session_id\": sid}}\n",
    ")\n",
    "print(resp1.content)\n",
    "\n",
    "resp2 = chain_with_memory.invoke(\n",
    "    {\"input\": \"Say my name.\"},\n",
    "    config={\"configurable\": {\"session_id\": sid}}\n",
    ")\n",
    "print(resp2.content)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Heisenberg! That's quite a memorable name. Are you a fan of the show, or is there another reason you go by that name?\n",
      "Heisenberg! You’ve definitely made a name for yourself. What’s on your mind today?\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "5b001aa00eca05a8",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Summary memory – instead of the full history, passes a condensed summary of previous conversations to the model, which saves tokens and makes it easier to scale long dialogues."
   ]
  },
  {
   "cell_type": "code",
   "id": "8aeae48dc35063df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:46:34.293984Z",
     "start_time": "2025-11-04T06:46:29.899632Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "summarizer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Summarize the following conversation briefly:\"),\n",
    "    (\"human\", \"{conversation}\")\n",
    "])\n",
    "summarizer = summarizer_prompt | llm | StrOutputParser()\n",
    "\n",
    "conversation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Here is the summary of prior conversation:\\n{summary}\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "conversation_chain = conversation_prompt | llm | StrOutputParser()\n",
    "\n",
    "store = {}\n",
    "summaries = {}\n",
    "\n",
    "def get_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "def get_summary(session_id: str) -> str:\n",
    "    if session_id not in summaries:\n",
    "        summaries[session_id] = \"No previous conversation.\"\n",
    "    return summaries[session_id]\n",
    "\n",
    "def update_summary(session_id: str, threshold: int = 4):\n",
    "    \"\"\"Update summary and clear old messages when threshold is reached\"\"\"\n",
    "    history = get_history(session_id)\n",
    "\n",
    "    if len(history.messages) >= threshold:\n",
    "        current_summary = summaries.get(session_id, \"\")\n",
    "\n",
    "        if current_summary and current_summary != \"No previous conversation.\":\n",
    "            conversation_text = f\"Previous summary: {current_summary}\\n\\nRecent messages: {history.messages}\"\n",
    "        else:\n",
    "            conversation_text = str(history.messages)\n",
    "\n",
    "        new_summary = summarizer.invoke({\"conversation\": conversation_text})\n",
    "        summaries[session_id] = new_summary\n",
    "\n",
    "        history.clear()\n",
    "\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    conversation_chain,\n",
    "    get_session_history=get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "sid = \"demo-summary\"\n",
    "\n",
    "cfg = {\"configurable\": {\"session_id\": sid}}\n",
    "summary = get_summary(sid)\n",
    "response1 = chain_with_memory.invoke({\"input\": \"Hello! My name is Michał.\", \"summary\": summary}, cfg)\n",
    "print(response1)\n",
    "\n",
    "# Update summary periodically (e.g., after every 2-3 exchanges)\n",
    "update_summary(sid, threshold=4)\n",
    "\n",
    "summary = get_summary(sid)\n",
    "response2 = chain_with_memory.invoke({\"input\": \"What is my name?\", \"summary\": summary}, cfg)\n",
    "print(response2)\n",
    "\n",
    "update_summary(sid, threshold=4)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Michał! How can I assist you today?\n",
      "Your name is Michał. How can I help you today?\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
