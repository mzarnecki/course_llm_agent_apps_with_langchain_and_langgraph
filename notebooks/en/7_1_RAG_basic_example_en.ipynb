{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18aff68d57dfa9f",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abbfab0c5f365cf",
   "metadata": {},
   "source": [
    "### Instalacja bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "534b948b01e54cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:50:04.631007Z",
     "start_time": "2025-10-13T06:50:02.940545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_text_splitters in /home/michal/anaconda3/lib/python3.13/site-packages (0.3.11)\r\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /home/michal/anaconda3/lib/python3.13/site-packages (from langchain_text_splitters) (0.3.76)\r\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /home/michal/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (0.4.30)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/michal/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (9.0.0)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/michal/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (1.33)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/michal/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (6.0.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/michal/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (4.15.0)\r\n",
      "Requirement already satisfied: packaging>=23.2 in /home/michal/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (24.2)\r\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /home/michal/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (2.11.7)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/michal/anaconda3/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (3.0.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/michal/.local/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (0.28.1)\r\n",
      "Requirement already satisfied: orjson>=3.9.14 in /home/michal/anaconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (3.10.14)\r\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/michal/anaconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (1.0.0)\r\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/michal/anaconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (2.32.5)\r\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/michal/anaconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (0.23.0)\r\n",
      "Requirement already satisfied: anyio in /home/michal/.local/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (4.10.0)\r\n",
      "Requirement already satisfied: certifi in /home/michal/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (2025.8.3)\r\n",
      "Requirement already satisfied: httpcore==1.* in /home/michal/.local/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (1.0.9)\r\n",
      "Requirement already satisfied: idna in /home/michal/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (3.7)\r\n",
      "Requirement already satisfied: h11>=0.16 in /home/michal/.local/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (0.16.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/michal/anaconda3/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/michal/anaconda3/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (2.33.2)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/michal/anaconda3/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (0.4.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/michal/anaconda3/lib/python3.13/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (3.3.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/michal/anaconda3/lib/python3.13/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (2.5.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/michal/.local/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (1.3.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_text_splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d5442171267e3",
   "metadata": {},
   "source": [
    "### Budowa VectorStore (FAISS) and Retrievera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:52:47.190355Z",
     "start_time": "2025-10-13T06:52:45.138388Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 7\n",
      "Znaleziony fragment:\n",
      "1. Retriever służy do wyszukiwania najbardziej podobnych dokumentów do zapytania użytkownika. Retriever\n",
      "2. Retriever może zwrócić różną liczbę pasujących dokumentów określona w parametrze k. Retriever\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "docs = [\n",
    "    \"LangChain this framework to pracy with LLM.\",\n",
    "    \"RAG łączy retrieval kontekstu with generacją answers.\",\n",
    "    \"FAISS this biblioteka to przechowywania and wyszukiwania embeddingów.\",\n",
    "    \"Retriever służy to wyszukiwania najbardziej podobnych dokumentów to queries użytkownika. Retriever może zwrócić różną liczbę pasujących dokumentów określona in parametrze k. Retriever wykorzystuje różne algorytmy podobieństwa tekstów, np. dopasowanie kosinusowe, odległość euklidesowa, MMR.\"\n",
    "]\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "split = splitter.create_documents(docs)\n",
    "\n",
    "print(f\"Number of chunks: {len(split)}\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(split, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "query = \"After co używa się retrievera?\"\n",
    "\n",
    "context = retriever.invoke(query)\n",
    "print(\"Znaleziony chunk:\")\n",
    "for i, c in enumerate(context, 1):\n",
    "    print(f\"{and}.\", c.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b755be95529a1",
   "metadata": {},
   "source": [
    "### Prosty chain RAG (prompt + kontekst + LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a500f01ecc94fa8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:54:10.919946Z",
     "start_time": "2025-10-13T06:54:08.766865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS to biblioteka do przechowywania i wyszukiwania embeddingów. Służy do efektywnego wyszukiwania najbardziej podobnych dokumentów do zapytania użytkownika.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Udziel precyzyjnej answers wyłącznie on podstawie KONTEKSTU. If brak danych — powiedz, that nie wiesz.\"),\n",
    "    (\"system\", \"KONTEKST:\\\\n{context}\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(rag_chain.invoke(\"Czym jest FAISS and to czego służy?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99540ca4f91ca9c2",
   "metadata": {},
   "source": [
    "### Example RAG - cały program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c3a659781bccd03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:55:44.418397Z",
     "start_time": "2025-10-13T06:55:41.630536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS to biblioteka do przechowywania i przeszukiwania dużych zbiorów wektorów.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#  Model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "#  Documents źródłowe\n",
    "docs = [\n",
    "    \"LangChain this framework to pracy with LLM.\",\n",
    "    \"RAG łączy dopasowanie kontekstu with generacją answers.\",\n",
    "    \"FAISS this biblioteka to przechowywania and wyszukiwania embeddingów.\"\n",
    "]\n",
    "\n",
    "#  Split\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n",
    "splits = splitter.create_documents(docs)\n",
    "\n",
    "#  Embeddings + vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "#  Prompt RAG\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Odpowiadaj tylko on podstawie kontekstu:\\\\n{context}\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "#  Pipeline\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "        \"question\": lambda x: x[\"question\"]\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "print(rag_chain.invoke({\"question\": \"Co this jest FAISS?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa40afa881954fcd",
   "metadata": {},
   "source": [
    "### RAG with pętlą and ewaluacją"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf6b17249a0ef1a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:56:39.303270Z",
     "start_time": "2025-10-13T06:56:36.078165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wynik ewaluacji Yes.\n",
      "✅ Odpowiedź zaakceptowana:\n",
      "RAG łączy dopasowanie kontekstu z generacją odpowiedzi.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "eval_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Oceń answer.\"),\n",
    "    (\"user\", \"Question: {question}\\\\nOdpowiedź: {answer}\\\\nCzy answer jest poprawna? Odpowiedz yes/no.\")\n",
    "])\n",
    "\n",
    "def rag_with_eval(question, max_retries):\n",
    "    for attempt in range(max_retries):\n",
    "        context = retriever.invoke(question)\n",
    "        answer = (prompt | llm | StrOutputParser()).invoke({\"context\": context, \"question\": question})\n",
    "        eval_result = (eval_prompt | llm | StrOutputParser()).invoke({\"question\": question, \"answer\": answer})\n",
    "        print(f\"Result ewaluacji {eval_result}\")\n",
    "        if \"yes\" in eval_result.lower():\n",
    "            return f\"✅ Answer zaakceptowana:\\\\n{answer}\"\n",
    "        print(f\"❌ Answer: {answer}\\\\n odrzucona, ponawiam próbę...\")\n",
    "    return \"Nie udało się uzyskać poprawnej answers.\"\n",
    "\n",
    "print(rag_with_eval(\"Co this jest RAG?\", max_retries=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
