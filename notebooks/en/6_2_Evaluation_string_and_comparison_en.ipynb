{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8843f9eae98bb3a4",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## String and comparison evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "68ff02d966926bc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T06:11:08.436915965Z",
     "start_time": "2025-12-19T06:11:08.363974212Z"
    }
   },
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "d97458f8894ba508",
   "metadata": {},
   "source": [
    "### Embedding Distance Evaluator\n",
    "Embedding Distance Evaluator compares two responses by converting them into embedding vectors and measuring distance or cosine similarity. This allows it to assess semantic content proximity, not just word matching."
   ]
  },
  {
   "cell_type": "code",
   "id": "abf878369f056ddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T06:11:17.864350345Z",
     "start_time": "2025-12-19T06:11:08.461251984Z"
    }
   },
   "source": [
    "from langchain_classic.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"embedding_distance\", embeddings_model=\"openai\")\n",
    "\n",
    "result1 = evaluator.evaluate_strings(\n",
    "    prediction=\"The capital of Poland is Warsaw\",\n",
    "    reference=\"The capital of Poland is Warsaw\"\n",
    ")\n",
    "\n",
    "result2 = evaluator.evaluate_strings(\n",
    "    prediction=\"The capital of Poland is Warsaw\",\n",
    "    reference=\"The capital of Poland is called Warsaw\"\n",
    ")\n",
    "\n",
    "result3 = evaluator.evaluate_strings(\n",
    "    prediction=\"The capital of Poland is Warsaw\",\n",
    "    reference=\"The capital of Burkina Faso is called Ouagadougou\"\n",
    ")\n",
    "\n",
    "print(round(result1[\"score\"], 4))\n",
    "print(round(result2[\"score\"], 4))\n",
    "print(round(result3[\"score\"], 4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0168\n",
      "0.1829\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "f51fedf1acbb83eb",
   "metadata": {},
   "source": [
    "### String Comparison\n",
    "The evaluator compares two texts using the BLEU metric, which measures the n-gram similarity of the generated answer to the reference answer."
   ]
  },
  {
   "cell_type": "code",
   "id": "b056b0e4a33539e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T06:11:18.062761883Z",
     "start_time": "2025-12-19T06:11:17.925236729Z"
    }
   },
   "source": [
    "evaluator = load_evaluator(\"string_distance\")\n",
    "\n",
    "result1 = evaluator.evaluate_strings(\n",
    "    prediction=\"The capital of Poland is Warsaw\",\n",
    "    reference=\"The capital of Poland is Warsaw\"\n",
    ")\n",
    "\n",
    "result2 = evaluator.evaluate_strings(\n",
    "    prediction=\"The capital of Poland is Warsaw\",\n",
    "    reference=\"The capital of Poland = Warsaw\"\n",
    ")\n",
    "\n",
    "result3 = evaluator.evaluate_strings(\n",
    "    prediction=\"The capital of Poland is Warsaw\",\n",
    "    reference=\"Warsaw is the capital of Poland\"\n",
    ")\n",
    "\n",
    "print(round(result1[\"score\"], 4))\n",
    "print(round(result2[\"score\"], 4))\n",
    "print(round(result3[\"score\"], 4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0334\n",
      "0.289\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "1c0bc677b17e562e",
   "metadata": {},
   "source": [
    "### A/B Tests\n",
    "PairwiseStringEvaluator compares two text responses against a single reference to determine which one is better. This allows you to automatically evaluate which response is closer to the expected result."
   ]
  },
  {
   "cell_type": "code",
   "id": "99cf12d4d38fb415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T06:11:24.269437022Z",
     "start_time": "2025-12-19T06:11:18.073349453Z"
    }
   },
   "source": [
    "from langchain_classic.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"labeled_pairwise_string\")\n",
    "\n",
    "result = evaluator.evaluate_string_pairs(\n",
    "    input=\"What is the capital of Poland?\",\n",
    "    prediction=\"Warsaw is the capital of Poland\",\n",
    "    prediction_b=\"I don't know\",\n",
    "    reference=\"Warsaw is Poland's capital\"\n",
    ")\n",
    "\n",
    "print(json.dumps(result, indent=4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"reasoning\": \"Assistant A's response is helpful, relevant, correct, and accurate. It directly answers the user's question about the capital of Poland. On the other hand, Assistant B's response is not helpful or accurate. It does not provide the user with the information they were seeking. Therefore, Assistant A's response is superior in this case. \\n\\nFinal verdict: [[A]]\",\n",
      "    \"value\": \"A\",\n",
      "    \"score\": 1\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
