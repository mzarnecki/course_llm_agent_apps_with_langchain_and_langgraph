{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain 1.X.X+ – New Syntax: Create_Agent, Messages, Structured Output, Memory, Middleware, Streaming, MCP\n",
    "\n",
    "This notebook is an **additional supplement** to the course – it demonstrates several key elements of the new LangChain (1.x) syntax, particularly the API around `create_agent`.\n",
    "The course uses examples using langchain_classic in many places, which can be replaced by the simplified LangChain syntax in versions above 1.x."
   ],
   "id": "15234553dfc72ae"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Instalacja bibliotek",
   "id": "7ebcee7b434ca1fe"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T19:13:01.503391905Z",
     "start_time": "2026-01-01T19:12:59.793129301Z"
    }
   },
   "source": "!pip install -q langchain python-dotenv langchain_mcp_adapters fastmcp",
   "id": "aec4a0e85b079adb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T19:13:01.599444871Z",
     "start_time": "2026-01-01T19:13:01.505643781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "50ec4267dd9f57f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_agent\n",
    "\n",
    "In the new syntax, the agent is created by `create_agent(model=..., tools=[...], ...)` and invoked by `invoke()`."
   ],
   "id": "76139e9b5a11b788"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T19:13:16.460935527Z",
     "start_time": "2026-01-01T19:13:01.601957037Z"
    }
   },
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "def rate_city(city: str) -> str:\n",
    "    \"\"\"Rate the city.\"\"\"\n",
    "    return f\"{city} is the best place in the world!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-mini\",\n",
    "    tools=[rate_city],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Is Poznań a nice city?.\"}]})\n",
    "\n",
    "last_msg = result[\"messages\"][-1]\n",
    "print(last_msg.content)\n"
   ],
   "id": "620d6212a116498",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short answer: yes — Poznań is widely regarded as a very pleasant city to visit, study in, and live in.\n",
      "\n",
      "Quick overview\n",
      "- Historic and compact: a beautifully preserved Old Town (Stary Rynek) with colorful merchant houses, the Town Hall with the famous billy goats (koziołki), and the cathedral on Ostrów Tumski.\n",
      "- Cultural life: good museums, theaters, regular festivals and events (and the Poznań International Fair brings plenty of business and conference activity).\n",
      "- Student city: several universities (e.g., Adam Mickiewicz University) give it a lively, youthful atmosphere and decent nightlife.\n",
      "- Parks and outdoors: Citadel Park, Malta Lake (water sports, walking/running routes), and green neighborhoods make it easy to get outside.\n",
      "- Transport and accessibility: reliable trams and buses, a regional airport with European connections, and good train links to Warsaw, Berlin, and other cities.\n",
      "- Food and local flavor: try rogale świętomarcińskie (St. Martin’s croissants) and local cuisine; growing café and restaurant scene.\n",
      "\n",
      "Pros\n",
      "- More affordable than Warsaw or Kraków on average (rent, eating out).\n",
      "- Compact and walkable — easy to explore on foot or by bike.\n",
      "- Strong local economy (trade fairs, industry, IT, startups).\n",
      "- Friendly, down-to-earth locals.\n",
      "\n",
      "Cons / things to be aware of\n",
      "- Winters can be cold and gray.\n",
      "- Smaller international scene than Warsaw — fewer direct intercontinental flights and some international services.\n",
      "- Air quality can dip occasionally (like many Polish cities).\n",
      "- During big trade fairs the centre can get busy.\n",
      "\n",
      "If you’re visiting: don’t miss Stary Rynek, Ostrów Tumski cathedral, the Imperial Castle, Citadel Park, and the Malta area. Try the local rogale and take a stroll or tram ride through Jeżyce for a more local vibe.\n",
      "\n",
      "If you’re moving or studying: popular residential areas include Jeżyce, Grunwald, Łazarz and parts of Winogrady — each has a different feel (family-oriented, student-friendly, hip cafés, etc.). Cost of living is moderate and the job market is strong in several sectors.\n",
      "\n",
      "Want tailored advice? Are you planning a short visit, studying there, or thinking about moving? Tell me your plans and I’ll give more specific recommendations.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages – Message Objects\n",
    "\n",
    "LangChain standardizes messages as objects (`SystemMessage`, `HumanMessage`, `AIMessage`, `ToolMessage`) and allows them to be used with chat models."
   ],
   "id": "bf46922bfbd6d84f"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T19:13:20.194551675Z",
     "start_time": "2026-01-01T19:13:16.520351100Z"
    }
   },
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.messages import SystemMessage, HumanMessage\n",
    "\n",
    "chat = init_chat_model(\"gpt-5-mini\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a concise assistant.\"),\n",
    "    HumanMessage(\"Write a 1-sentence summary of what LangChain is.\"),\n",
    "]\n",
    "\n",
    "ai_msg = chat.invoke(messages)  # -> AIMessage\n",
    "print(ai_msg.content)"
   ],
   "id": "72fe78325f6db316",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open-source framework for building applications with large language models by providing modular components—prompts, chains, agents, memory, and integrations—that connect LLMs to data, tools, and workflows.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured output – `response_format` in `create_agent`\n",
    "\n",
    "Instead of parsing text, you can ask the agent to return data that conforms to a type (e.g., a Pydantic model).\n",
    "The result is then sent to `result[\"structured_response\"]`."
   ],
   "id": "3a8e8b879fcf2030"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T19:13:23.793950002Z",
     "start_time": "2026-01-01T19:13:20.337794802Z"
    }
   },
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    \"\"\"Contact information for a person.\"\"\"\n",
    "    name: str = Field(description=\"The name of the person\")\n",
    "    email: str = Field(description=\"The email address\")\n",
    "    phone: str = Field(description=\"The phone number\")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-mini\",\n",
    "    response_format=ContactInfo,\n",
    ")\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}\n",
    "    ]\n",
    "})\n",
    "\n",
    "structured = result[\"structured_response\"]\n",
    "print(structured)\n",
    "print(type(structured))\n"
   ],
   "id": "815d89d7f5a0553f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='John Doe' email='john@example.com' phone='(555) 123-4567'\n",
      "<class '__main__.ContactInfo'>\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short‑term memory\n",
    "\n",
    "In LangChain 1.X.X, *short‑term* memory is implemented by a dedicated InMemorySaver component.\n",
    "\n",
    "- `InMemorySaver()` keeps state in the process's memory (ideal for notebooks)\n",
    "- `thread_id` identifies the \"conversation thread\" (important for multiple `invoke()`)\n"
   ],
   "id": "4d218551f2252cae"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T19:13:29.235616924Z",
     "start_time": "2026-01-01T19:13:23.843827024Z"
    }
   },
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-mini\",\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-thread-1\"}}\n",
    "\n",
    "agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Michael.\"}]}, config=config)\n",
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is my name?\"}]}, config=config)\n",
    "print(result[\"messages\"][-1].content)\n"
   ],
   "id": "ebe3710f4e4ad35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Michael.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middleware - Human‑in‑the‑loop middleware\n",
    "\n",
    "Middleware allows you to \"hook in\" to the agent's execution.\n",
    "Typical scenario: the agent can *read* data, but actions like *sending email/writing to the database* require approval.\n",
    "Human‑in‑the‑loop – stopping before running selected tools and resuming with `Command(resume=...)`."
   ],
   "id": "208b7a469b1fe5bf"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T19:13:34.647983296Z",
     "start_time": "2026-01-01T19:13:29.283638756Z"
    }
   },
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.types import Command\n",
    "\n",
    "def read_email(email_id: str) -> str:\n",
    "    \"\"\"Read email mock function\"\"\"\n",
    "    return f\"(mock) Email content for id={email_id}\"\n",
    "\n",
    "def send_email(recipient: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Send email mock function\"\"\"\n",
    "    return f\"(mock) Sent email to {recipient} with subject={subject} and content={body}\"\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-mini\",\n",
    "    tools=[read_email, send_email],\n",
    "    checkpointer=checkpointer,\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                \"send_email\": {\"allowed_decisions\": [\"approve\", \"edit\", \"reject\"]},\n",
    "                \"read_email\": False,\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"hitl-demo\"}}\n",
    "\n",
    "paused = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Send an email to alice@example.com with subject 'Hi' and say hello.\"}]},\n",
    "    config=config,\n",
    ")\n",
    "print(\"Paused state keys:\", paused.keys())"
   ],
   "id": "ed8943c512ff4890",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paused state keys: dict_keys(['messages', '__interrupt__'])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T19:13:35.975166276Z",
     "start_time": "2026-01-01T19:13:34.699389785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "resumed = agent.invoke(\n",
    "    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n",
    "    config=config,\n",
    ")\n",
    "print(resumed[\"messages\"][-1].content)"
   ],
   "id": "3c43e143b7baceb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done — I sent the email to alice@example.com with subject \"Hi\" and body \"Hello.\"\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middleware - Guardrails – PII middleware\n",
    "\n",
    "We can also use guardrails as middleware, for example, to protect against sensitive data leaks in LLM responses."
   ],
   "id": "aa530874f89ba981"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T19:13:43.060982716Z",
     "start_time": "2026-01-01T19:13:36.024812906Z"
    }
   },
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import PIIMiddleware\n",
    "\n",
    "def echo(text: str) -> str:\n",
    "    \"\"\"Print text.\"\"\"\n",
    "    return text\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-mini\",\n",
    "    tools=[echo],\n",
    "    middleware=[\n",
    "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n",
    "        PIIMiddleware(\"credit_card\", strategy=\"mask\", apply_to_input=True),\n",
    "        PIIMiddleware(\n",
    "            \"api_key\",\n",
    "            detector=r\"sk-[a-zA-Z0-9]{32}\",\n",
    "            strategy=\"block\",\n",
    "            apply_to_input=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "out = agent.invoke({\n",
    "    \"messages\": [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Extract information from text: My email is john@example.com and card is 5105-1051-0510-5100\"\n",
    "    }]\n",
    "})\n",
    "print(out[\"messages\"][-1].content)\n"
   ],
   "id": "817a34949d01b675",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"email\": \"[REDACTED_EMAIL]\",\n",
      "  \"card_masked\": \"****-****-****-5100\",\n",
      "  \"card_last4\": \"5100\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "In LangChain, we can return partial responses or subsequent generated tokens \"on the fly\":\n",
    "- **agent progress** (`stream_mode=\"updates\"`) – event after each step\n",
    "- **tokens/model messages** (`stream_mode=\"messages\"`) – UI-friendly"
   ],
   "id": "c7328314668ba532"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T19:14:03.721799327Z",
     "start_time": "2026-01-01T19:13:43.139077981Z"
    }
   },
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "def rate_city(city: str) -> str:\n",
    "    \"\"\"Rate city mock tool.\"\"\"\n",
    "    return f\"The best city is {city}!\"\n",
    "\n",
    "agent = create_agent(model=\"gpt-5\", tools=[rate_city])\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Is Poznań a nice city?  Rate city and afterwards plan a trip to Poznań in 5 stages.\"}]},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    for step, data in chunk.items():\n",
    "        last = data[\"messages\"][-1]\n",
    "        print(f\"step: {step:>6} | type={type(last).__name__}\")\n",
    "        try:\n",
    "            print(\"content_blocks:\", last.content_blocks)\n",
    "        except Exception:\n",
    "            print(\"content:\", getattr(last, \"content\", None))\n"
   ],
   "id": "bff4b1e2ae1e6262",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  model | type=AIMessage\n",
      "content_blocks: [{'type': 'tool_call', 'id': 'call_WFpN8AvSTKhA2c38YpgK6zxG', 'name': 'rate_city', 'args': {'city': 'Poznań'}}]\n",
      "step:  tools | type=ToolMessage\n",
      "content_blocks: [{'type': 'text', 'text': 'The best city is Poznań!'}]\n",
      "step:  model | type=AIMessage\n",
      "content_blocks: [{'type': 'text', 'text': 'Yes—many visitors and residents consider Poznań a very nice city. Quick highlights and caveats:\\n\\nPros\\n- Beautiful Old Town with a Renaissance town hall and lively Stary Rynek.\\n- Green spaces like Cytadela Park and Lake Malta, plus Cathedral Island.\\n- Vibrant food and café scene (try St. Martin croissants) and good craft beer.\\n- Student energy, cultural events (Malta Festival, St. Martin’s Day), and solid museums.\\n- Generally affordable compared to Warsaw/Kraków, with efficient trams and good cycling paths.\\n\\nCons\\n- Winters can be gray and chilly; occasional air-quality dips.\\n- English is common in the center but less so in outer neighborhoods.\\n- Fewer “blockbuster” sights than Kraków or Gdańsk.\\n\\nGood for a weekend or longer stay, family-friendly (zoo, Palm House), and a base for day trips (Kórnik Castle, Rogalin Palace, Wielkopolski National Park). Ideal seasons: late spring to early autumn.'}]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCP – Model Context Protocol\n",
    "\n",
    "MCP standardizes the use of external tools through LLM.\n",
    "At LangChain, we use the `langchain-mcp-adapters` adapter and the `MultiServerMCPClient` client, which can retrieve a list of tools from multiple servers."
   ],
   "id": "96fecd1bd18c3101"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimal MCP server (FastMCP)\n",
    "Save the code below as `math_server.py` in same folder as this notebook."
   ],
   "id": "8bbfe203a981dbee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T19:14:05.118460164Z",
     "start_time": "2026-01-01T19:14:05.036118748Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "from fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Math\")\n",
    "\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"Add two numbers\"\n",
    "    return a + b\n",
    "\n",
    "@mcp.tool()\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"Multiply two numbers\"\n",
    "    return a * b\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"stdio\")\n",
    "```"
   ],
   "id": "21cff0693925f472"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "and run cell below:",
   "id": "584d7c2f76680d12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T19:27:33.163910125Z",
     "start_time": "2026-01-01T19:27:28.742498596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def demo_mcp():\n",
    "    client = MultiServerMCPClient(\n",
    "        {\n",
    "            \"math\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"math_server.py\"],\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    tools = await client.get_tools()\n",
    "    agent = create_agent(\"gpt-5-mini\", tools)\n",
    "\n",
    "    r1 = await agent.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]})\n",
    "    print(r1[\"messages\"][-1].content)\n",
    "\n",
    "# Uncomment below to run\n",
    "# asyncio.run(demo_mcp())"
   ],
   "id": "40fd024097248f60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3 + 5) = 8, and 8 × 12 = 96.\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
