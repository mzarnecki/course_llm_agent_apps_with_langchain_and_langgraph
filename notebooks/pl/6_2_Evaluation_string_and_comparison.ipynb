{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8843f9eae98bb3a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## String and comparison evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ff02d966926bc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T06:57:59.720738Z",
     "start_time": "2025-11-07T06:57:59.711910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97458f8894ba508",
   "metadata": {},
   "source": [
    "### Embedding Distance Evaluator\n",
    "Embedding Distance Evaluator porównuje dwie odpowiedzi, zamieniając je na wektory osadzeń (embeddings) i mierząc odległość lub podobieństwo kosinusowe. Dzięki temu ocenia semantyczną bliskość treści, a nie tylko dopasowanie słów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf878369f056ddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T06:58:04.609604Z",
     "start_time": "2025-11-07T06:57:59.785940Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/course_llm_agent_apps_with_langchain_and_langgraph/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n",
      "0.0508\n",
      "0.1318\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"embedding_distance\", embeddings_model=\"openai\")\n",
    "\n",
    "result1 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolica Polski to Warszawa\",\n",
    "    reference=\"Stolica Polski to Warszawa\"\n",
    ")\n",
    "\n",
    "result2 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolica Polski to Warszawa\",\n",
    "    reference=\"Stolica Polski nosi nazwę Warszawa\"\n",
    ")\n",
    "\n",
    "result3 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolica Polski to Warszawa\",\n",
    "    reference=\"Stolica Burkina Faso nosi nazwę Wagadugu\"\n",
    ")\n",
    "\n",
    "print(round(result1[\"score\"], 4))\n",
    "print(round(result2[\"score\"], 4))\n",
    "print(round(result3[\"score\"], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51fedf1acbb83eb",
   "metadata": {},
   "source": [
    "### String Comparison\n",
    "Ewaluator porównuje dwa teksty przy użyciu metryki BLEU, która mierzy n-gramowe podobieństwo wygenerowanej odpowiedzi do odpowiedzi referencyjnej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b056b0e4a33539e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T06:58:04.711467Z",
     "start_time": "2025-11-07T06:58:04.672950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.069\n",
      "0.4991\n"
     ]
    }
   ],
   "source": [
    "evaluator = load_evaluator(\"string_distance\", metric=\"bleu\")\n",
    "\n",
    "result1 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolica Polski to Warszawa\",\n",
    "    reference=\"Stolica Polski to Warszawa\"\n",
    ")\n",
    "\n",
    "result2 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolicą Polski jest Warszawa\",\n",
    "    reference=\"Stolica Polski to Warszawa\"\n",
    ")\n",
    "\n",
    "result3 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolica Polski to Warszawa\",\n",
    "    reference=\"Warszawa jest stolicą Polski\"\n",
    ")\n",
    "\n",
    "print(round(result1[\"score\"], 4))\n",
    "print(round(result2[\"score\"], 4))\n",
    "print(round(result3[\"score\"], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b33b6e594d30c36",
   "metadata": {},
   "source": [
    "### String Comparison: BLUE, ROUGE, METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c360ac20053cc1cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T06:58:04.732099Z",
     "start_time": "2025-11-07T06:58:04.727363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: {'score': 0.28903225806451616}\n",
      "ROUGE: {'score': 0.11385199240986721}\n",
      "METEOR: {'score': 0.30186335403726705}\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.evaluation import load_evaluator\n",
    "\n",
    "# BLEU evaluator\n",
    "bleu_eval = load_evaluator(\"string_distance\", metric=\"bleu\")\n",
    "\n",
    "result_bleu = bleu_eval.evaluate_strings(\n",
    "    prediction=\"Warsaw is the capital of Poland\",\n",
    "    reference=\"The capital of Poland is Warsaw\"\n",
    ")\n",
    "print(\"BLEU:\", result_bleu)\n",
    "\n",
    "# ROUGE evaluator\n",
    "rouge_eval = load_evaluator(\"string_distance\", metric=\"rouge\")\n",
    "\n",
    "result_rouge = rouge_eval.evaluate_strings(\n",
    "    prediction=\"Warsaw is capital\",\n",
    "    reference=\"Warsaw is the capital of Poland\"\n",
    ")\n",
    "print(\"ROUGE:\", result_rouge)\n",
    "\n",
    "# METEOR evaluator\n",
    "meteor_eval = load_evaluator(\"string_distance\", metric=\"meteor\")\n",
    "\n",
    "result_meteor = meteor_eval.evaluate_strings(\n",
    "    prediction=\"The dog runs quickly\",\n",
    "    reference=\"The dog is running fast\"\n",
    ")\n",
    "print(\"METEOR:\", result_meteor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0bc677b17e562e",
   "metadata": {},
   "source": [
    "### Testy A/B\n",
    "PairwiseStringEvaluator służy do porównywania dwóch odpowiedzi tekstowych względem jednej referencji, aby wybrać lepszą. Dzięki temu można automatycznie ocenić, która z odpowiedzi jest bliższa oczekiwanemu wynikowi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99cf12d4d38fb415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T06:58:07.049039Z",
     "start_time": "2025-11-07T06:58:04.780284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"reasoning\": \"Assistant A's response is helpful, relevant, correct, and accurate. It directly answers the user's question about the capital of Poland. On the other hand, Assistant B's response is not helpful or accurate. It does not provide the user with the information they were seeking. Therefore, Assistant A's response is superior in this case. \\n\\nFinal verdict: [[A]]\",\n",
      "    \"value\": \"A\",\n",
      "    \"score\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"labeled_pairwise_string\")\n",
    "\n",
    "result = evaluator.evaluate_string_pairs(\n",
    "    input=\"What is the capital of Poland?\",\n",
    "    prediction=\"Warsaw is the capital of Poland\",\n",
    "    prediction_b=\"I don't know\",\n",
    "    reference=\"Warsaw is Poland's capital\"\n",
    ")\n",
    "\n",
    "print(json.dumps(result, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
