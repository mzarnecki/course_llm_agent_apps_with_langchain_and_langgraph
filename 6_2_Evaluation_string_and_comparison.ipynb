{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "## String and comparison evaluation",
   "id": "8843f9eae98bb3a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T06:35:35.089027Z",
     "start_time": "2025-09-15T06:35:32.363651Z"
    }
   },
   "cell_type": "code",
   "source": "! pip install rapidfuzz",
   "id": "6d15ef0340fd7c95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rapidfuzz\r\n",
      "  Downloading rapidfuzz-3.14.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\r\n",
      "Downloading rapidfuzz-3.14.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.2/3.2 MB\u001B[0m \u001B[31m10.9 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: rapidfuzz\r\n",
      "Successfully installed rapidfuzz-3.14.1\r\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T06:34:14.040549Z",
     "start_time": "2025-09-15T06:34:14.038053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### LangSmith (tracing) — opcjonalnie\n",
    "# włącz śledzenie (jeśli masz konto)\n",
    "# Opcjonalnie (wymaga konta):\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = \"<TWÓJ_KLUCZ>\"\n",
    "# os.environ[\"LANGSMITH_PROJECT\"] = \"kurs-demo\"\n",
    "print(\"LangSmith: ustaw zmienne środowiskowe, aby włączyć tracing.\")"
   ],
   "id": "68ff02d966926bc4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith: ustaw zmienne środowiskowe, aby włączyć tracing.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Embedding Distance Evaluator",
   "id": "d97458f8894ba508"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T06:34:41.719999Z",
     "start_time": "2025-09-15T06:34:40.804890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "evaluator = load_evaluator(\"embedding_distance\", embeddings_model=\"openai\")\n",
    "\n",
    "result = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolica Polski to Warszawa\",\n",
    "    reference=\"Warszawa jest stolicą Polski\"\n",
    ")\n",
    "\n",
    "print(result)\n"
   ],
   "id": "abf878369f056ddc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.055613485077632974}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### String Comparison Evaluator",
   "id": "f51fedf1acbb83eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T06:35:42.091585Z",
     "start_time": "2025-09-15T06:35:42.076843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluator = load_evaluator(\"string_distance\", metric=\"bleu\")\n",
    "\n",
    "result = evaluator.evaluate_strings(\n",
    "    prediction=\"Warsaw is the capital of Poland\",\n",
    "    reference=\"The capital of Poland is Warsaw\"\n",
    ")\n",
    "\n",
    "print(result)\n"
   ],
   "id": "b056b0e4a33539e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.28903225806451616}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T06:35:46.379652Z",
     "start_time": "2025-09-15T06:35:46.374399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# BLEU evaluator\n",
    "bleu_eval = load_evaluator(\"string_distance\", metric=\"bleu\")\n",
    "\n",
    "result_bleu = bleu_eval.evaluate_strings(\n",
    "    prediction=\"Warsaw is the capital of Poland\",\n",
    "    reference=\"The capital of Poland is Warsaw\"\n",
    ")\n",
    "print(\"BLEU:\", result_bleu)\n",
    "\n",
    "# ROUGE evaluator\n",
    "rouge_eval = load_evaluator(\"string_distance\", metric=\"rouge\")\n",
    "\n",
    "result_rouge = rouge_eval.evaluate_strings(\n",
    "    prediction=\"Warsaw is capital\",\n",
    "    reference=\"Warsaw is the capital of Poland\"\n",
    ")\n",
    "print(\"ROUGE:\", result_rouge)\n",
    "\n",
    "# METEOR evaluator\n",
    "meteor_eval = load_evaluator(\"string_distance\", metric=\"meteor\")\n",
    "\n",
    "result_meteor = meteor_eval.evaluate_strings(\n",
    "    prediction=\"The dog runs quickly\",\n",
    "    reference=\"The dog is running fast\"\n",
    ")\n",
    "print(\"METEOR:\", result_meteor)\n"
   ],
   "id": "c360ac20053cc1cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: {'score': 0.28903225806451616}\n",
      "ROUGE: {'score': 0.11385199240986721}\n",
      "METEOR: {'score': 0.30186335403726705}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### A/B Testing",
   "id": "1c0bc677b17e562e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.evaluation import PairwiseStringEvaluator\n",
    "\n",
    "evaluator = PairwiseStringEvaluator()\n",
    "\n",
    "result = evaluator.evaluate_string_pairs(\n",
    "    prediction=\"Warsaw is the capital of Poland\",\n",
    "    prediction_b=\"The capital city of Poland is Warsaw\",\n",
    "    reference=\"Warsaw is Poland's capital\"\n",
    ")\n",
    "\n",
    "print(result)  # np. {'winner': 'prediction_b', 'score': 0.9}\n"
   ],
   "id": "99cf12d4d38fb415"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluate LLM by LLM",
   "id": "498d46b178a8c225"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T06:36:03.270394Z",
     "start_time": "2025-09-15T06:36:03.207868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Evaluate LLM by LLM\n",
    "# before start fill env variables .env file:\n",
    "# LANGCHAIN_API_KEY=\"put_here_your_langchain(langsmith)_api_token\"\n",
    "# OPENAI_API_KEY=\"put_here_your_openai_token\"\n",
    "# HUGGINGFACE_API_TOKEN=\"put_here_your_huggingface_token\"\n",
    "# to use OpenAI API you need to add billing details https://platform.openai.com/settings/organization/billing/overview\n",
    "# for langchain token remember to add read permissions associated with token\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "#load dotenv (API key from .env)\n",
    "load_dotenv()"
   ],
   "id": "9f1adf5dd6ef7976",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T06:39:29.020508Z",
     "start_time": "2025-09-15T06:39:23.313724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "template = \"\"\"\n",
    "You are base of knowledge about star wars. Respond to question below with only name without any additional text.\n",
    "{input}\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "chain = prompt_template | llm\n",
    "prediction = chain.invoke({\"input\": \"What is the capital of star wars Sith Empire?\"})\n",
    "\n",
    "evaluator = load_evaluator(\"labeled_score_string\", llm=ChatOpenAI(model=\"gpt-4o\"))\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=prediction,\n",
    "    reference=\"Dromund Kaas\",\n",
    "    input=\"What is the capital of star wars Sith Empire?\",\n",
    ")\n",
    "print(eval_result)"
   ],
   "id": "fec00ff98e795156",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The AI assistant has responded with \"Dromund Kaas\" as the capital of the Sith Empire in the Star Wars universe. This response is correct and relevant. \"Dromund Kaas\" is indeed known as the capital of the Sith Empire, especially during periods depicted in the expanded universe materials such as the \"Star Wars: The Old Republic\" video game and associated lore. The response is also concise and directly answers the user\\'s question without unnecessary information, which can be seen as a lack of depth but is typically acceptable for straightforward factual queries. The assistant\\'s response does not provide additional context or background, which limits depth but maintains correctness and relevance.\\n\\nRating: [[9]]', 'score': 9}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T06:38:19.292825Z",
     "start_time": "2025-09-15T06:38:19.289523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = ChatOpenAI(temperature=0)\n",
    "template = \"\"\"You are an expert in grading answers.\n",
    "You are grading the following question:\n",
    "{query}\n",
    "Here is the correct expected answer:\n",
    "{answer}\n",
    "You are grading the following predicted answer:\n",
    "{result}\n",
    "What grade do you give from 0 to 5, where 0 is the lowest for low similarity and 5 is for the high similarity?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\", \"result\"], template=template\n",
    ")"
   ],
   "id": "b348fa1178dc88f2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T06:38:25.223347Z",
     "start_time": "2025-09-15T06:38:22.517337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_examples = [\n",
    "    {\n",
    "        \"question\": \"Why people don't brief underwater?\",\n",
    "        \"context\": \"Because people don't have gills\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Why the sky is blue?\",\n",
    "        \"context\": \"Sky isn't blue. Its just optical effect related to sun rays coming to eye through atmosphere and interpretation in our mind.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is in my pocket?\",\n",
    "        \"context\": \"\",\n",
    "    },\n",
    "]\n",
    "prompt_qa = \"Answer the question based on the  context\\nContext:{context}\\nQuestion:{question}\\nAnswer:\"\n",
    "template = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_qa)\n",
    "qa_chain = LLMChain(llm=model, prompt=template)\n",
    "predictions = qa_chain.apply(context_examples)\n",
    "predictions"
   ],
   "id": "1ced50c2cc0a4f8e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Because people don't have gills, they are not able to breathe underwater.\"},\n",
       " {'text': 'The sky appears blue due to an optical effect caused by sun rays passing through the atmosphere and being interpreted by our eyes and mind.'},\n",
       " {'text': \"I'm sorry, I cannot answer that question as I do not have the ability to see or know what is in your pocket.\"}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T06:36:27.447444Z",
     "start_time": "2025-09-15T06:36:27.071745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.evaluation.qa import ContextQAEvalChain\n",
    "\n",
    "eval_chain = ContextQAEvalChain.from_llm(model)\n",
    "graded_outputs = eval_chain.evaluate(\n",
    "    context_examples, predictions, question_key=\"question\", prediction_key=\"text\"\n",
    ")\n",
    "print(graded_outputs)"
   ],
   "id": "72ae1694f489b76f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': ' CORRECT'}, {'text': ' CORRECT'}, {'text': ' CORRECT'}]\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
