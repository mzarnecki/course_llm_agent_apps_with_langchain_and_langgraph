{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "## String and comparison evaluation",
   "id": "8843f9eae98bb3a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T06:57:59.720738Z",
     "start_time": "2025-11-07T06:57:59.711910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "68ff02d966926bc4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Embedding Distance Evaluator\n",
    "Embedding Distance Evaluator porównuje dwie odpowiedzi, zamieniając je na wektory osadzeń (embeddings) i mierząc odległość lub podobieństwo kosinusowe. Dzięki temu ocenia semantyczną bliskość treści, a nie tylko dopasowanie słów."
   ],
   "id": "d97458f8894ba508"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T06:58:04.609604Z",
     "start_time": "2025-11-07T06:57:59.785940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_classic.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"embedding_distance\", embeddings_model=\"openai\")\n",
    "\n",
    "result1 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolica Polski to Warszawa\",\n",
    "    reference=\"Stolica Polski to Warszawa\"\n",
    ")\n",
    "\n",
    "result2 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolica Polski to Warszawa\",\n",
    "    reference=\"Stolica Polski nosi nazwę Warszawa\"\n",
    ")\n",
    "\n",
    "result3 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolica Polski to Warszawa\",\n",
    "    reference=\"Stolica Burkina Faso nosi nazwę Wagadugu\"\n",
    ")\n",
    "\n",
    "print(round(result1[\"score\"], 4))\n",
    "print(round(result2[\"score\"], 4))\n",
    "print(round(result3[\"score\"], 4))\n"
   ],
   "id": "abf878369f056ddc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n",
      "0.0508\n",
      "0.1318\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### String Comparison\n",
    "Ewaluator porównuje dwa teksty przy użyciu metryki BLEU, która mierzy n-gramowe podobieństwo wygenerowanej odpowiedzi do odpowiedzi referencyjnej."
   ],
   "id": "f51fedf1acbb83eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T06:58:04.711467Z",
     "start_time": "2025-11-07T06:58:04.672950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluator = load_evaluator(\"string_distance\", metric=\"bleu\")\n",
    "\n",
    "result1 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolica Polski to Warszawa\",\n",
    "    reference=\"Stolica Polski to Warszawa\"\n",
    ")\n",
    "\n",
    "result2 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolicą Polski jest Warszawa\",\n",
    "    reference=\"Stolica Polski to Warszawa\"\n",
    ")\n",
    "\n",
    "result3 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolica Polski to Warszawa\",\n",
    "    reference=\"Warszawa jest stolicą Polski\"\n",
    ")\n",
    "\n",
    "print(round(result1[\"score\"], 4))\n",
    "print(round(result2[\"score\"], 4))\n",
    "print(round(result3[\"score\"], 4))\n"
   ],
   "id": "b056b0e4a33539e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.069\n",
      "0.4991\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### String Comparison: BLUE, ROUGE, METEOR",
   "id": "8b33b6e594d30c36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T06:58:04.732099Z",
     "start_time": "2025-11-07T06:58:04.727363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_classic.evaluation import load_evaluator\n",
    "\n",
    "# BLEU evaluator\n",
    "bleu_eval = load_evaluator(\"string_distance\", metric=\"bleu\")\n",
    "\n",
    "result_bleu = bleu_eval.evaluate_strings(\n",
    "    prediction=\"Warsaw is the capital of Poland\",\n",
    "    reference=\"The capital of Poland is Warsaw\"\n",
    ")\n",
    "print(\"BLEU:\", result_bleu)\n",
    "\n",
    "# ROUGE evaluator\n",
    "rouge_eval = load_evaluator(\"string_distance\", metric=\"rouge\")\n",
    "\n",
    "result_rouge = rouge_eval.evaluate_strings(\n",
    "    prediction=\"Warsaw is capital\",\n",
    "    reference=\"Warsaw is the capital of Poland\"\n",
    ")\n",
    "print(\"ROUGE:\", result_rouge)\n",
    "\n",
    "# METEOR evaluator\n",
    "meteor_eval = load_evaluator(\"string_distance\", metric=\"meteor\")\n",
    "\n",
    "result_meteor = meteor_eval.evaluate_strings(\n",
    "    prediction=\"The dog runs quickly\",\n",
    "    reference=\"The dog is running fast\"\n",
    ")\n",
    "print(\"METEOR:\", result_meteor)\n"
   ],
   "id": "c360ac20053cc1cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: {'score': 0.28903225806451616}\n",
      "ROUGE: {'score': 0.11385199240986721}\n",
      "METEOR: {'score': 0.30186335403726705}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Testy A/B\n",
    "PairwiseStringEvaluator służy do porównywania dwóch odpowiedzi tekstowych względem jednej referencji, aby wybrać lepszą. Dzięki temu można automatycznie ocenić, która z odpowiedzi jest bliższa oczekiwanemu wynikowi."
   ],
   "id": "1c0bc677b17e562e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T06:58:07.049039Z",
     "start_time": "2025-11-07T06:58:04.780284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_classic.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"labeled_pairwise_string\")\n",
    "\n",
    "result = evaluator.evaluate_string_pairs(\n",
    "    input=\"What is the capital of Poland?\",\n",
    "    prediction=\"Warsaw is the capital of Poland\",\n",
    "    prediction_b=\"I don't know\",\n",
    "    reference=\"Warsaw is Poland's capital\"\n",
    ")\n",
    "\n",
    "print(json.dumps(result, indent=4))"
   ],
   "id": "99cf12d4d38fb415",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"reasoning\": \"Assistant A provided a correct, accurate, and factual response to the user's question. It was also relevant and helpful. On the other hand, Assistant B's response was not helpful, accurate, or relevant. It did not provide the user with the information they were seeking. Therefore, Assistant A's response is superior in this case. \\n\\nFinal verdict: [[A]]\",\n",
      "    \"value\": \"A\",\n",
      "    \"score\": 1\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
