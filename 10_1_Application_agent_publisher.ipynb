{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Agent wydawca",
   "id": "c02019f356bc7b77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install -q langgraph langchain langchain-openai langchain-community python-dotenv pydot\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Literal, Dict, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "# Model bazowy (możesz podmienić na inny, np. gpt-4o-mini / gpt-4.1-mini)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Czy mamy prawdziwy klucz do Tavily?\n",
    "HAS_TAVILY = bool(os.getenv(\"TAVILY_API_KEY\"))\n",
    "print(\"Model OK. Tavily:\", \"ON\" if HAS_TAVILY else \"mock\")\n"
   ],
   "id": "7e55c493e5f5d400"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stan aplikacji",
   "id": "7e586e809c9af44f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PublisherState(MessagesState):\n",
    "    # preferencje użytkownika (uczymy się z feedbacku)\n",
    "    sources: List[str] = []\n",
    "    topics: List[str] = []\n",
    "    # jaki tryb wybrał agent: \"daily\" | \"fun\" | \"single\"\n",
    "    mode: Literal[\"daily\", \"fun\", \"single\"] = \"daily\"\n",
    "    # ostatnie surowe wyniki z narzędzia (np. listy {title, url, content})\n",
    "    raw_results: List[Dict[str, Any]] = []\n",
    "    # skrócone podsumowanie w Markdown\n",
    "    summary_md: str = \"\"\n"
   ],
   "id": "e747674c30d3ae3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Narzędzia Tavily lub mock",
   "id": "a43d77f5f56c33ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_search_tool():\n",
    "    if HAS_TAVILY:\n",
    "        # Prawdziwe narzędzie (zwraca listę wyników wyszukiwania)\n",
    "        return TavilySearchResults(\n",
    "            max_results=10,\n",
    "            search_depth=\"advanced\",  # „advanced” lub „basic”\n",
    "            include_answer=False,\n",
    "            include_raw_content=False,\n",
    "        )\n",
    "    else:\n",
    "        # Lokalny mock — udaje wyniki wyszukiwania\n",
    "        from langchain.tools import tool\n",
    "\n",
    "        @tool(\"mock_search\", return_direct=False)\n",
    "        def mock_search(query: str) -> List[Dict[str, str]]:\n",
    "            \"\"\"MOCK: Zwraca przykładowe wyniki wyszukiwania (title, url, content).\"\"\"\n",
    "            base = \"https://example.com/\"\n",
    "            return [\n",
    "                {\"title\": \"AI & Startups Weekly\", \"url\": base + \"ai-startups\", \"content\": \"Roundup nowości AI w startupach.\"},\n",
    "                {\"title\": \"LangChain Updates\", \"url\": base + \"langchain\", \"content\": \"Nowe funkcje LC i ekosystem narzędzi.\"},\n",
    "                {\"title\": \"Funding News\", \"url\": base + \"funding\", \"content\": \"Rundy finansowania w branży technologicznej.\"},\n",
    "                {\"title\": \"ML Research Highlights\", \"url\": base + \"ml-paper\", \"content\": \"Przegląd ciekawych publikacji ML.\"},\n",
    "                {\"title\": \"Product Launches\", \"url\": base + \"launches\", \"content\": \"Nowe produkty i funkcje.\"},\n",
    "            ]\n",
    "        return mock_search\n",
    "\n",
    "search_tool = get_search_tool()\n",
    "tool_node = ToolNode([search_tool])\n"
   ],
   "id": "67572ff4de33528"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompty systemowe",
   "id": "732a2c73234961c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "AGENT_SYSTEM = \"\"\"Jesteś agentem wydawcą. Twoje zadania:\n",
    "1) Zrozumieć intencję użytkownika i wybrać tryb:\n",
    "   - daily: dzienne podsumowanie (5–8 linków),\n",
    "   - fun: jeden ciekawostkowy news + wyjaśnienie dlaczego interesujący,\n",
    "   - single: podsumowanie z jednego wskazanego źródła (1–3 linki).\n",
    "2) Jeśli potrzeba informacji, wywołaj narzędzie wyszukiwania.\n",
    "3) Gdy masz wystarczająco materiałów, zakończ fazę planowania — NIE twórz finalnej odpowiedzi (to zrobi summarizer).\n",
    "\n",
    "Zasady:\n",
    "- Jeśli użytkownik poda źródła lub tematy, traktuj je priorytetowo.\n",
    "- Jeśli pierwsze wyniki są słabe/za mało (mniej niż 5 do daily), spróbuj innego sformułowania zapytania.\n",
    "- Używaj zwięzłych narzędziowych zapytań.\n",
    "Zwróć decyzje poprzez tool_calls (jeśli trzeba szukać) lub od razu przejdź do podsumowania (bez tool_calls).\n",
    "\"\"\"\n",
    "\n",
    "SUMMARIZER_SYSTEM = \"\"\"Sformatuj wyniki jako **Markdown**:\n",
    "- dla 'daily': listy punktowane z krótkimi opisami, 5–8 pozycji\n",
    "- dla 'fun': 1–2 pozycje z akcentem na ciekawostkę i „dlaczego”\n",
    "- dla 'single': 1–3 pozycje z jednego źródła\n",
    "Na końcu dodaj sekcję **Źródła** jako listę numerowaną z URL.\n",
    "Nie wymyślaj linków — używaj tylko podanych.\n",
    "\"\"\"\n",
    "\n",
    "FEEDBACK_SYSTEM = \"\"\"Jesteś menedżerem preferencji. Na podstawie wiadomości użytkownika wyodrębnij:\n",
    "- sources: lista nazw/serwisów (np. 'TechCrunch', 'The Verge')\n",
    "- topics: lista tematów (np. 'AI', 'fintech', 'startup funding')\n",
    "Zwróć minimalny JSON: {\"sources\": [...], \"topics\": [...]}. Nic poza tym.\n",
    "\"\"\"\n"
   ],
   "id": "d286fc999db049d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Logika węzłów",
   "id": "b5b448d127848d36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "def agent_node(state: PublisherState) -> Command:\n",
    "    \"\"\"\n",
    "    Planner: decyduje o trybie (daily/fun/single), przygotowuje i ewentualnie\n",
    "    wywołuje narzędzie wyszukiwania (tool_calls). Jeśli uzna, że ma dość danych,\n",
    "    przejdzie do 'summarize' bez tool_calls.\n",
    "    \"\"\"\n",
    "    # Zbuduj wiadomości\n",
    "    msgs = [SystemMessage(content=AGENT_SYSTEM)]\n",
    "    if state.get(\"sources\"):\n",
    "        msgs.append(SystemMessage(content=f\"Preferowane źródła: {', '.join(state['sources'])}\"))\n",
    "    if state.get(\"topics\"):\n",
    "        msgs.append(SystemMessage(content=f\"Preferowane tematy: {', '.join(state['topics'])}\"))\n",
    "    msgs += state[\"messages\"]\n",
    "\n",
    "    # Podpowiedź dla modelu: wybierz tryb\n",
    "    mode_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Na podstawie ostatniej wiadomości użytkownika wybierz tryb: 'daily', 'fun' lub 'single'. \"\n",
    "        \"Odpowiedz jednym słowem.\"\n",
    "    )\n",
    "    mode_result = (mode_prompt | llm | StrOutputParser()).invoke(\n",
    "        {\"input\": \"\"}\n",
    "    ).strip().lower()\n",
    "    mode = \"daily\" if mode_result not in {\"daily\", \"fun\", \"single\"} else mode_result\n",
    "\n",
    "    # Zwiąż narzędzia i poproś model o decyzję/ew. tool_calls\n",
    "    llm_with_tools = llm.bind_tools([search_tool])\n",
    "\n",
    "    # Podstawowe zalecenie: jeśli daily/single – najpierw szukaj; gdy fun – też spróbuj wyszukania 1–2 pozycji\n",
    "    agent_decision = llm_with_tools.invoke(msgs)\n",
    "\n",
    "    # Uaktualnij stan (zapisz nową wiadomość i wybrany tryb)\n",
    "    update = {\"messages\": [agent_decision], \"mode\": mode}\n",
    "\n",
    "    # Jeśli są tool_calls → przejdź do tools, w przeciwnym razie → summarize\n",
    "    goto = \"tools\" if getattr(agent_decision, \"tool_calls\", None) else \"summarize\"\n",
    "    return Command(update=update, goto=goto)\n",
    "\n",
    "\n",
    "def summarize_node(state: PublisherState) -> PublisherState:\n",
    "    \"\"\"Buduje finalne podsumowanie na podstawie dostępnych wiadomości i wyników narzędzia.\"\"\"\n",
    "    # Wyciągnij najnowsze „surowe” wyniki z komunikatów narzędzi (dla Tavily są w AIMessage.tool_calls → ToolMessage)\n",
    "    # Uproszczenie: zlecamy modelowi zebranie źródeł z całej rozmowy.\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", SUMMARIZER_SYSTEM),\n",
    "        (\"user\", \"Tryb: {mode}\\nHistoria:\\n{history}\\n\\nUtwórz końcowe podsumowanie teraz.\")\n",
    "    ])\n",
    "    history_text = \"\\n\".join([f\"{m.type.upper()}: {m.content}\" for m in state[\"messages\"]])\n",
    "    summary = (prompt | llm | StrOutputParser()).invoke({\"mode\": state[\"mode\"], \"history\": history_text})\n",
    "    return {\"summary_md\": summary, \"messages\": [AIMessage(content=summary)]}\n",
    "\n",
    "\n",
    "def feedback_node(state: PublisherState) -> Command:\n",
    "    \"\"\"\n",
    "    Human-in-the-loop: poproś użytkownika o krótką informację zwrotną\n",
    "    dotyczącą preferowanych źródeł/tematów. W notebooku skorzystamy z interrupt(),\n",
    "    aby wpisać feedback ręcznie z interfejsu Jupytera.\n",
    "    \"\"\"\n",
    "    req = {\n",
    "        \"action_request\": {\n",
    "            \"action\": \"Podaj preferencje: źródła i/lub tematy, np. 'Źródła: TechCrunch, The Verge; Tematy: AI, startupy'. \"\n",
    "                      \"Możesz też wpisać 'pomiń'.\",\n",
    "            \"args\": {},\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"allow_ignore\": True,\n",
    "            \"allow_respond\": True,\n",
    "            \"allow_edit\": False,\n",
    "            \"allow_accept\": False,\n",
    "        },\n",
    "        \"description\": state.get(\"summary_md\", \"\"),\n",
    "    }\n",
    "    resp = interrupt([req])[0]\n",
    "\n",
    "    if resp[\"type\"] == \"response\":\n",
    "        user_text = resp[\"args\"]\n",
    "        # Zparsuj preferencje LLM-em do JSON\n",
    "        parser = JsonOutputParser()\n",
    "        pref_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", FEEDBACK_SYSTEM),\n",
    "            (\"user\", \"{feedback}\")\n",
    "        ])\n",
    "        parsed = (pref_prompt | llm | parser).invoke({\"feedback\": user_text})\n",
    "        # scal preferencje (unikalne)\n",
    "        new_sources = list({*(state.get(\"sources\", [])), *parsed.get(\"sources\", [])})\n",
    "        new_topics = list({*(state.get(\"topics\", [])), *parsed.get(\"topics\", [])})\n",
    "\n",
    "        update = {\"sources\": new_sources, \"topics\": new_topics, \"messages\": [HumanMessage(content=user_text)]}\n",
    "        goto = END\n",
    "        return Command(update=update, goto=goto)\n",
    "\n",
    "    # „ignore” lub inne → po prostu kończymy\n",
    "    return Command(goto=END)\n"
   ],
   "id": "426061570d01a390"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Budowa grafu",
   "id": "4b858b6f087deb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def should_continue_from_agent(state: PublisherState) -> str:\n",
    "    \"\"\"Jeśli ostatnia odpowiedź zawiera tool_calls → przejdź do tools; w przeciwnym razie → summarize.\"\"\"\n",
    "    last = state[\"messages\"][-1]\n",
    "    return \"tools\" if getattr(last, \"tool_calls\", None) else \"summarize\"\n",
    "\n",
    "graph = (\n",
    "    StateGraph(PublisherState)\n",
    "    .add_node(\"agent\", agent_node)\n",
    "    .add_node(\"tools\", tool_node)         # wykonanie tool_calls (np. wyszukiwania)\n",
    "    .add_node(\"summarize\", summarize_node)\n",
    "    .add_node(\"feedback\", feedback_node)\n",
    "    .add_edge(START, \"agent\")\n",
    "    .add_edge(\"tools\", \"agent\")           # po narzędziach wróć do agenta (może poprosić o kolejne wyszukiwanie lub podsumować)\n",
    "    .add_conditional_edges(\"agent\", should_continue_from_agent, {\"tools\": \"tools\", \"summarize\": \"summarize\"})\n",
    "    .add_edge(\"summarize\", \"feedback\")    # na końcu prosimy o feedback (HITL)\n",
    "    .add_edge(\"feedback\", END)\n",
    ")\n",
    "\n",
    "app = graph.compile()\n",
    "print(\"Graf skompilowany.\")\n"
   ],
   "id": "93a67d879af8a5c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Uruchomienie",
   "id": "b215aa4849ae8a86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "initial: PublisherState = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Zrób daily briefing o AI i startupach za ostatnie 48h. Preferowane: The Verge, TechCrunch.\")\n",
    "    ],\n",
    "    \"sources\": [],\n",
    "    \"topics\": [],\n",
    "    \"mode\": \"daily\",\n",
    "    \"raw_results\": [],\n",
    "    \"summary_md\": \"\"\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial)\n",
    "print(\"\\n=== PODSUMOWANIE (Markdown) ===\\n\")\n",
    "print(final_state.get(\"summary_md\", \"brak\"))\n"
   ],
   "id": "1c32acbbe8b6797d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wizualizacja grafu",
   "id": "660589ed22a19484"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "png_bytes = app.get_graph().draw_png()\n",
    "display(Image(png_bytes))\n"
   ],
   "id": "ee3bf0480d0c32f5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
