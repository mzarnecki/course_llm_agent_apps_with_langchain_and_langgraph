{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.3. Parametry API LLM",
   "id": "a84cf2306f684808"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### instalacja i konfiguracja",
   "id": "9d30c57b2be3c6d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "86a9a4cdc795ecbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain-openai python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### pomocnicza funkcja do testÃ³w",
   "id": "c8a2d22ae079dd8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def test_generation(param_name, values, prompt=\"WymyÅ›l krÃ³tkÄ… bajkÄ™ o kocie i psie.\"):\n",
    "    for v in values:\n",
    "        print(\"=\"*60)\n",
    "        print(f\"{param_name} = {v}\")\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # bazowe ustawienia\n",
    "        kwargs = {param_name: v}\n",
    "        response = llm.invoke(prompt, **kwargs)\n",
    "        print(response.content, \"\\n\")\n"
   ],
   "id": "e323e8af393d58ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### eksperyment z temperaturÄ…",
   "id": "440cf02cee488c9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_generation(\"temperature\", [0.0, 0.7, 1.2])\n",
   "id": "9259b3726ef39352"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### eksperyment z top_p",
   "id": "7be7d1c943c147c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_generation(\"top_p\", [0.2, 0.7, 1.0])\n",
   "id": "1de3bc2661c0da2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### eksperyment z top_k",
   "id": "a516c379475b4e7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_generation(\"top_k\", [1, 10, 50])\n",
   "id": "bcd96e246c68a9db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### eksperyment z max_tokens",
   "id": "65d7a10ab7906237"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_generation(\"max_tokens\", [30, 100, 300])\n",
   "id": "914687df9cd9ac31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ðŸ’¡ Efekt:\\\n",
    "przy temperature=0 odpowiedzi bÄ™dÄ… powtarzalne,\\\n",
    "przy temperature=1.2 â€” bardziej szalone,\\\n",
    "przy niskim top_p model bÄ™dzie zachowawczy,\\\n",
    "przy duÅ¼ym top_k â€” bardziej rÃ³Å¼norodny,\\\n",
    "a max_tokens zdecyduje, czy bajka ma 2 zdania, czy caÅ‚Ä… stronÄ™."
   ],
   "id": "779bcca8320a55c7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
