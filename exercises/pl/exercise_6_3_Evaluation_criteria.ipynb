{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc2feef8",
   "metadata": {},
   "source": "## Zadania: Ewaluacja za pomocą kryteriów\n"
  },
  {
   "cell_type": "markdown",
   "id": "465af146",
   "metadata": {},
   "source": [
    "Zadanie 1 \\\n",
    "Słownik kryteriów \\\n",
    "Do ponizszej listy dodaj dodatkowe kryteria i zakresy oceny."
   ]
  },
  {
   "cell_type": "code",
   "id": "3c157bfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:12:41.750830Z",
     "start_time": "2025-11-11T17:12:41.747760Z"
    }
   },
   "source": [
    "criteria = {\n",
    "    \"correctness\": {\"desc\": \"Czy odpowiedź jest merytorycznie poprawna?\", \"scale\": (0,5)},\n",
    "    \"conciseness\": {\"desc\": \"Czy odpowiedź jest zwięzła i na temat?\", \"scale\": (0,5)},\n",
    "    \"usefulness\": {\"desc\": \"Czy odpowiedź jest praktycznie przydatna?\", \"scale\": (0,5)},\n",
    "}\n",
    "assert all(k in criteria for k in [\"correctness\",\"conciseness\",\"usefulness\"])\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "1e36f507",
   "metadata": {},
   "source": [
    "Zadanie 2 \\\n",
    "Zmień w ponizszym kodzie parametry criteria (np. na \"helpfulness\") oraz prediction i input na własne teksty do porównania."
   ]
  },
  {
   "cell_type": "code",
   "id": "2ce987fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:16:56.072474Z",
     "start_time": "2025-11-11T17:16:52.410963Z"
    }
   },
   "source": [
    "from langchain_classic.evaluation import load_evaluator\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 1) Użyjemy ewaluatora z referencją (labeled_criteria)\n",
    "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\")\n",
    "\n",
    "# 2) Porównujemy odpowiedź modelu z referencją\n",
    "result = evaluator.evaluate_strings(\n",
    "    prediction=\"2 + 2 = 4\",\n",
    "    input=\"Policz 2 + 2\",\n",
    "    reference=\"4\",\n",
    ")\n",
    "\n",
    "print(json.dumps(result, indent=4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"reasoning\": \"The criterion for this task is the correctness of the submission. The input asks to calculate 2 + 2. The submission provided is 2 + 2 = 4. The reference answer is 4. \\n\\nComparing the submission and the reference, it is clear that the submission is correct as it matches the reference answer. Therefore, the submission meets the criterion of correctness. \\n\\nY\",\n",
      "    \"value\": \"Y\",\n",
      "    \"score\": 1\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
