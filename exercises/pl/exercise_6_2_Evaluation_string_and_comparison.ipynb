{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a691cf5",
   "metadata": {},
   "source": "## Zadania: Ewaluacja - porównanie łańcuchów tekstowych\n"
  },
  {
   "cell_type": "markdown",
   "id": "4f0709af",
   "metadata": {},
   "source": [
    "Zadanie 1 \\\n",
    "BLEU, ROUGE, METEOR \\\n",
    "Uruchom poniższy kod i porównaj wyniki metryk dla porównania tych samych 2 tekstów za pomoca róznych metryk.\n",
    "Zwróć uwagę na różnice wyników i konieczność wykorzystania tej samej metryki do pporównania tekstów."
   ]
  },
  {
   "cell_type": "code",
   "id": "a1ac1427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:29:39.622154Z",
     "start_time": "2025-11-11T16:29:33.842467Z"
    }
   },
   "source": [
    "! pip install sacrebleu rouge-score nltk sacrebleu rouge-score\n",
    "\n",
    "from sacrebleu.metrics import BLEU\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "prediction = \"Warsaw is the capital of Poland\"\n",
    "reference = \"The capital of Poland is Warsaw\"\n",
    "\n",
    "# BLEU\n",
    "bleu = BLEU(effective_order=True)  # better behavior on short texts\n",
    "bleu_score = bleu.corpus_score([prediction], [[reference]]).score / 100.0  # normalize 0-1\n",
    "\n",
    "# ROUGE\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "rouge_scores = scorer.score(reference, prediction)\n",
    "rougeL_f = rouge_scores[\"rougeL\"].fmeasure\n",
    "\n",
    "# METEOR\n",
    "meteor = meteor_score([reference.split()], prediction.split())\n",
    "\n",
    "print(f\"BLEU: {bleu_score:.4f}\")\n",
    "print(f\"ROUGE-L (F1): {rougeL_f:.4f}\")\n",
    "print(f\"METEOR: {meteor:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\r\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\r\n",
      "Collecting rouge-score\r\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: nltk in /home/michal/anaconda3/lib/python3.13/site-packages (3.9.1)\r\n",
      "Collecting portalocker (from sacrebleu)\r\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\r\n",
      "Requirement already satisfied: regex in /home/michal/anaconda3/lib/python3.13/site-packages (from sacrebleu) (2024.11.6)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/michal/anaconda3/lib/python3.13/site-packages (from sacrebleu) (0.9.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/michal/anaconda3/lib/python3.13/site-packages (from sacrebleu) (2.2.5)\r\n",
      "Requirement already satisfied: colorama in /home/michal/anaconda3/lib/python3.13/site-packages (from sacrebleu) (0.4.6)\r\n",
      "Requirement already satisfied: lxml in /home/michal/anaconda3/lib/python3.13/site-packages (from sacrebleu) (5.3.0)\r\n",
      "Collecting absl-py (from rouge-score)\r\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /home/michal/anaconda3/lib/python3.13/site-packages (from rouge-score) (1.17.0)\r\n",
      "Requirement already satisfied: click in /home/michal/anaconda3/lib/python3.13/site-packages (from nltk) (8.1.8)\r\n",
      "Requirement already satisfied: joblib in /home/michal/anaconda3/lib/python3.13/site-packages (from nltk) (1.5.1)\r\n",
      "Requirement already satisfied: tqdm in /home/michal/anaconda3/lib/python3.13/site-packages (from nltk) (4.67.1)\r\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\r\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\r\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\r\n",
      "Building wheels for collected packages: rouge-score\r\n",
      "  Building wheel for rouge-score (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=a84033bb60c7c6c6ff9d6290336a7b9c24f07ea82cd0f04254dff05316de05a7\r\n",
      "  Stored in directory: /home/michal/.cache/pip/wheels/44/af/da/5ffc433e2786f0b1a9c6f458d5fb8f611d8eb332387f18698f\r\n",
      "Successfully built rouge-score\r\n",
      "Installing collected packages: portalocker, absl-py, sacrebleu, rouge-score\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4/4\u001B[0m [rouge-score]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed absl-py-2.3.1 portalocker-3.2.0 rouge-score-0.1.2 sacrebleu-2.5.1\r\n",
      "BLEU: 0.3433\n",
      "ROUGE-L (F1): 0.6667\n",
      "METEOR: 0.9375\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "8213ce02",
   "metadata": {},
   "source": [
    "Zadanie 2 \\\n",
    "Testy A/B \\\n",
    "Podmień teksty w ponizszym przykładzie na własne i sprawdź który zostanie wybrany przez model."
   ]
  },
  {
   "cell_type": "code",
   "id": "497ea63b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:20:08.493049Z",
     "start_time": "2025-11-11T16:20:04.617003Z"
    }
   },
   "source": [
    "from langchain_classic.evaluation import load_evaluator\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "evaluator = load_evaluator(\"labeled_pairwise_string\")\n",
    "\n",
    "result = evaluator.evaluate_string_pairs(\n",
    "    input=\"What is the capital of Poland?\",\n",
    "    prediction=\"Warsaw is the capital of Poland\",\n",
    "    prediction_b=\"I don't know\",\n",
    "    reference=\"Warsaw is Poland's capital\"\n",
    ")\n",
    "\n",
    "print(json.dumps(result, indent=4))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"reasoning\": \"Assistant A provided a correct, accurate, and factual answer to the user's question, demonstrating helpfulness and relevance. On the other hand, Assistant B's response was not helpful or relevant, as it did not provide the information the user was seeking. Therefore, Assistant A's response is superior in this case. \\n\\nFinal Verdict: [[A]]\",\n",
      "    \"value\": \"A\",\n",
      "    \"score\": 1\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Zadanie 3 \\\n",
    "Embedding Distance Evaluator \\\n",
    "Uruchom ponizszy kod i odpowiedz na pytanie: \\\n",
    "Kiedy dystans embedding jest największy - jeżeli teksty są pdobne, czy jeżeli teskty znacznie różnia sie od siebie?"
   ],
   "id": "d55adac82cc029de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:20:16.193047Z",
     "start_time": "2025-11-11T16:20:12.324249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_classic.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"embedding_distance\", embeddings_model=\"openai\")\n",
    "\n",
    "result1 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolica Polski to Warszawa\",\n",
    "    reference=\"Stolica Polski to Warszawa\"\n",
    ")\n",
    "\n",
    "result2 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolica Polski to Warszawa\",\n",
    "    reference=\"Stolica Polski nosi nazwę Warszawa\"\n",
    ")\n",
    "\n",
    "result3 = evaluator.evaluate_strings(\n",
    "    prediction=\"Stolica Polski to Warszawa\",\n",
    "    reference=\"Stolica Burkina Faso nosi nazwę Wagadugu\"\n",
    ")\n",
    "\n",
    "print(round(result1[\"score\"], 4))\n",
    "print(round(result2[\"score\"], 4))\n",
    "print(round(result3[\"score\"], 4))"
   ],
   "id": "3c4fd84710d3d7ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n",
      "0.0508\n",
      "0.1318\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
