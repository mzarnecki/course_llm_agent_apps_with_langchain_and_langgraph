{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cf1986",
   "metadata": {},
   "source": [
    "## Zadania: LangChain: Chat z pamięcią\n",
    "\n",
    "> **Wskazówka:** miejsca do uzupełnienia oznaczono jako `# TODO` lub `...`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae15552",
   "metadata": {},
   "source": "## Zadanie 1 - `RunnableWithMessageHistory`: opakuj łańcuch o pamięć"
  },
  {
   "cell_type": "code",
   "id": "44b7960e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T09:55:33.697192Z",
     "start_time": "2025-10-04T09:55:33.521723Z"
    }
   },
   "source": [
    "# Cel: Zbuduj prosty łańcuch z pamięcią konwersacji.\n",
    "# Odniesienie: 2_4_LangChain_chat_with_memory.ipynb\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Upewnij się, że masz ustawione OPENAI_API_KEY w środowisku.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # NIE wstawiaj klucza do repo\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # TODO: możesz zmienić model\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Jesteś asystentem. Pamiętaj kontekst rozmowy.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm  # TODO: prosta kompozycja\n",
    "\n",
    "store = {}\n",
    "def get_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# (Nie wywołujemy realnie modelu w testach); Sprawdź, czy obiekt istnieje.\n",
    "assert chain_with_memory is not None, \"Nie udało się zbudować łańcucha z pamięcią.\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "edfa9ce0",
   "metadata": {},
   "source": [
    "## Zadanie 2 - Pamięć rozmowy: dodaj bufor rozmowy do łańcucha\n",
    "*Uwaga:* interfejs pamięci w LangChain ewoluuje. Poniższy szkic pokazuje ideę z buforem rozmów."
   ]
  },
  {
   "cell_type": "code",
   "id": "1302d76f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T09:55:36.779976Z",
     "start_time": "2025-10-04T09:55:36.719917Z"
    }
   },
   "source": [
    "# Cel: Zademonstruj integrację bufora historii z łańcuchem (wersja poglądowa).\n",
    "# Jeśli Twoja wersja LangChain oferuje nowsze podejście, użyj go.\n",
    "\n",
    "try:\n",
    "    from langchain.memory import ConversationBufferMemory  # API może się różnić w wersjach\n",
    "    memory = ConversationBufferMemory(return_messages=True)\n",
    "    memory.save_context({\"input\": \"Hej!\"}, {\"output\": \"Cześć, w czym pomóc?\"})\n",
    "    hist = memory.load_memory_variables({})\n",
    "    print(\"Historia:\", hist)\n",
    "except Exception as e:\n",
    "    print(\"Pominięto demonstrację ConversationBufferMemory (inne API?):\", e)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historia: {'history': [HumanMessage(content='Hej!', additional_kwargs={}, response_metadata={}), AIMessage(content='Cześć, w czym pomóc?', additional_kwargs={}, response_metadata={})]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21449/1815453156.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
