{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c758963",
   "metadata": {},
   "source": [
    "# Zadania â€“ 3_1 LangChain Chains\n",
    "\n",
    "*Prepared: 2025-10-04 10:00*\n",
    "\n",
    "Budowa rÃ³Å¼nych typÃ³w chains: prosty, sekwencyjny, rÃ³wnolegÅ‚y i miniâ€‘RAG.\n",
    "\n",
    "> **Instrukcja:** Zadania sÄ… krÃ³tkie i polegajÄ… na uzupeÅ‚nieniu brakujÄ…cych fragmentÃ³w kodu (`____`). NiektÃ³re komÃ³rki zawierajÄ… komentarze `# TODO` i `# HINT`. Uruchamiaj komÃ³rki po kolei."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec9df30",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Wymagania\n",
    "- `langchain_core`, `langchain_openai`, `langchain` â‰¥ 0.2\n",
    "- Opcjonalnie: `faiss-cpu`, `tiktoken`, `python-dotenv`\n",
    "\n",
    "JeÅ›li nie masz klucza API, moÅ¼esz uruchamiaÄ‡ komÃ³rki â€žna suchoâ€ â€“ celem jest *uzupeÅ‚nienie kodu*, a nie koniecznie jego wykonanie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a5858a",
   "metadata": {},
   "source": [
    "### Zadanie 1: Prosty chain (Prompt âžœ LLM)\n",
    "UzupeÅ‚nij brakujÄ…ce elementy, aby zbudowaÄ‡ najprostszy Å‚aÅ„cuch, ktÃ³ry odpowiada grzecznie po polsku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: UzupeÅ‚nij importy i miejsca oznaczone ____\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser  # opcjonalnie\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # lub inny model\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"JesteÅ› pomocnym asystentem. Odpowiadaj po polsku.\"),\n",
    "    (\"user\", \"{pytanie}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm   # | StrOutputParser()  # opcjonalnie\n",
    "\n",
    "# Oczekiwany efekt: odpowiedÅº po polsku\n",
    "wynik = chain.invoke({\"pytanie\": \"W jednym zdaniu: czym jest LangChain?\"})\n",
    "print(type(wynik), \"\\n\", wynik)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5667f624",
   "metadata": {},
   "source": [
    "### Zadanie 2: Sekwencyjny chain\n",
    "StwÃ³rz Å‚aÅ„cuch dwuetapowy: (1) parafraza, (2) skrÃ³cenie do 1 zdania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Zbuduj chain sekwencyjny prompt1 -> llm -> prompt2 -> llm\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"PrzeksztaÅ‚Ä‡ wypowiedÅº uÅ¼ytkownika w bardziej formalny styl. Po polsku.\"),\n",
    "    (\"user\", \"{tekst}\")\n",
    "])\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"StreÅ›Ä‡ poprzedni tekst do jednego zdania. Po polsku.\"),\n",
    "    (\"user\", \"{wejscie}\")\n",
    "])\n",
    "\n",
    "# HINT: UÅ¼yj operatora | i mapowania kluczy za pomocÄ… .map() lub prostego lambda\n",
    "stage1 = prompt1 | llm\n",
    "# poniÅ¼ej wstaw przekazanie wyniku stage1 do prompt2 (klucz 'wejscie'):\n",
    "stage2_input_map = lambda x: {\"wejscie\": x.content if hasattr(x, \"content\") else str(x)}\n",
    "\n",
    "chain = stage1 | stage2_input_map | (prompt2 | llm)\n",
    "\n",
    "print(chain.invoke({\"tekst\": \"LangChain pozwala budowaÄ‡ zÅ‚oÅ¼one przepÅ‚ywy z LLM.\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f69dc4",
   "metadata": {},
   "source": [
    "### Zadanie 3: Miniâ€‘RAG chain (retriever âžœ kontekst âžœ odpowiedÅº)\n",
    "UzupeÅ‚nij brakujÄ…ce fragmenty, aby zbudowaÄ‡ prosty Å‚aÅ„cuch RAG z lokalnym indeksem FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30230bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Zbuduj miniâ€‘RAG. JeÅ›li nie masz faiss, moÅ¼esz tylko uzupeÅ‚niÄ‡ kod bez uruchamiania.\n",
    "# HINT: Teksty -> embeddingi -> wektorowy index -> retriever -> prompt -> llm\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "try:\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    ok = True\n",
    "except Exception as e:\n",
    "    print(\"Brak FAISS, ale moÅ¼esz uzupeÅ‚niÄ‡ kod. Info:\", e)\n",
    "    ok = False\n",
    "\n",
    "docs = [\n",
    "    \"LangChain pozwala Å‚Ä…czyÄ‡ modele z narzÄ™dziami i pamiÄ™ciÄ….\",\n",
    "    \"RAG to wzorzec: retrieval augmented generation.\",\n",
    "    \"FAISS to biblioteka do wyszukiwania podobnych wektorÃ³w.\"\n",
    "]\n",
    "\n",
    "emb = OpenAIEmbeddings()  # wymaga API; traktuj jako placeholder\n",
    "if ok:\n",
    "    db = FAISS.from_texts(docs, embedding=emb)\n",
    "    retriever = db.as_retriever(k=2)\n",
    "else:\n",
    "    retriever = lambda q: []  # placeholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"UÅ¼yj podanego kontekstu do odpowiedzi. Odpowiadaj po polsku.\"),\n",
    "    (\"system\", \"Kontekst: {context}\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def build_context(question: str):\n",
    "    # HINT: wywoÅ‚aj retriever i poÅ‚Ä…cz dokumenty w jeden string\n",
    "    ctx_docs = retriever(question) if callable(retriever) else []\n",
    "    if hasattr(ctx_docs, \"__iter__\"):\n",
    "        texts = [getattr(d, \"page_content\", str(d)) for d in ctx_docs]\n",
    "    else:\n",
    "        texts = []\n",
    "    return \"\\n---\\n\".join(texts) if texts else \"Brak kontekstu (placeholder).\"\n",
    "\n",
    "rag = (\n",
    "    {\"context\": lambda x: build_context(x[\"question\"]), \"question\": lambda x: x[\"question\"]}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "print(rag.invoke({\"question\": \"Czym jest RAG i do czego sÅ‚uÅ¼y FAISS?\"}))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
