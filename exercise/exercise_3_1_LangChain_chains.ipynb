{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c758963",
   "metadata": {},
   "source": [
    "# Zadania – 3_1 LangChain Chains\n",
    "\n",
    "*Prepared: 2025-10-04 10:00*\n",
    "\n",
    "Budowa różnych typów chains: prosty, sekwencyjny, równoległy i mini‑RAG.\n",
    "\n",
    "> **Instrukcja:** Zadania są krótkie i polegają na uzupełnieniu brakujących fragmentów kodu (`____`). Niektóre komórki zawierają komentarze `# TODO` i `# HINT`. Uruchamiaj komórki po kolei."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec9df30",
   "metadata": {},
   "source": [
    "## 📦 Wymagania\n",
    "- `langchain_core`, `langchain_openai`, `langchain` ≥ 0.2\n",
    "- Opcjonalnie: `faiss-cpu`, `tiktoken`, `python-dotenv`\n",
    "\n",
    "Jeśli nie masz klucza API, możesz uruchamiać komórki „na sucho” – celem jest *uzupełnienie kodu*, a nie koniecznie jego wykonanie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a5858a",
   "metadata": {},
   "source": [
    "### Zadanie 1: Prosty chain (Prompt ➜ LLM)\n",
    "Uzupełnij brakujące elementy, aby zbudować najprostszy łańcuch, który odpowiada grzecznie po polsku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Uzupełnij importy i miejsca oznaczone ____\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser  # opcjonalnie\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # lub inny model\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Jesteś pomocnym asystentem. Odpowiadaj po polsku.\"),\n",
    "    (\"user\", \"{pytanie}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm   # | StrOutputParser()  # opcjonalnie\n",
    "\n",
    "# Oczekiwany efekt: odpowiedź po polsku\n",
    "wynik = chain.invoke({\"pytanie\": \"W jednym zdaniu: czym jest LangChain?\"})\n",
    "print(type(wynik), \"\\n\", wynik)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5667f624",
   "metadata": {},
   "source": [
    "### Zadanie 2: Sekwencyjny chain\n",
    "Stwórz łańcuch dwuetapowy: (1) parafraza, (2) skrócenie do 1 zdania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Zbuduj chain sekwencyjny prompt1 -> llm -> prompt2 -> llm\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Przekształć wypowiedź użytkownika w bardziej formalny styl. Po polsku.\"),\n",
    "    (\"user\", \"{tekst}\")\n",
    "])\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Streść poprzedni tekst do jednego zdania. Po polsku.\"),\n",
    "    (\"user\", \"{wejscie}\")\n",
    "])\n",
    "\n",
    "# HINT: Użyj operatora | i mapowania kluczy za pomocą .map() lub prostego lambda\n",
    "stage1 = prompt1 | llm\n",
    "# poniżej wstaw przekazanie wyniku stage1 do prompt2 (klucz 'wejscie'):\n",
    "stage2_input_map = lambda x: {\"wejscie\": x.content if hasattr(x, \"content\") else str(x)}\n",
    "\n",
    "chain = stage1 | stage2_input_map | (prompt2 | llm)\n",
    "\n",
    "print(chain.invoke({\"tekst\": \"LangChain pozwala budować złożone przepływy z LLM.\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f69dc4",
   "metadata": {},
   "source": [
    "### Zadanie 3: Mini‑RAG chain (retriever ➜ kontekst ➜ odpowiedź)\n",
    "Uzupełnij brakujące fragmenty, aby zbudować prosty łańcuch RAG z lokalnym indeksem FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30230bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Zbuduj mini‑RAG. Jeśli nie masz faiss, możesz tylko uzupełnić kod bez uruchamiania.\n",
    "# HINT: Teksty -> embeddingi -> wektorowy index -> retriever -> prompt -> llm\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "try:\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    ok = True\n",
    "except Exception as e:\n",
    "    print(\"Brak FAISS, ale możesz uzupełnić kod. Info:\", e)\n",
    "    ok = False\n",
    "\n",
    "docs = [\n",
    "    \"LangChain pozwala łączyć modele z narzędziami i pamięcią.\",\n",
    "    \"RAG to wzorzec: retrieval augmented generation.\",\n",
    "    \"FAISS to biblioteka do wyszukiwania podobnych wektorów.\"\n",
    "]\n",
    "\n",
    "emb = OpenAIEmbeddings()  # wymaga API; traktuj jako placeholder\n",
    "if ok:\n",
    "    db = FAISS.from_texts(docs, embedding=emb)\n",
    "    retriever = db.as_retriever(k=2)\n",
    "else:\n",
    "    retriever = lambda q: []  # placeholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Użyj podanego kontekstu do odpowiedzi. Odpowiadaj po polsku.\"),\n",
    "    (\"system\", \"Kontekst: {context}\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def build_context(question: str):\n",
    "    # HINT: wywołaj retriever i połącz dokumenty w jeden string\n",
    "    ctx_docs = retriever(question) if callable(retriever) else []\n",
    "    if hasattr(ctx_docs, \"__iter__\"):\n",
    "        texts = [getattr(d, \"page_content\", str(d)) for d in ctx_docs]\n",
    "    else:\n",
    "        texts = []\n",
    "    return \"\\n---\\n\".join(texts) if texts else \"Brak kontekstu (placeholder).\"\n",
    "\n",
    "rag = (\n",
    "    {\"context\": lambda x: build_context(x[\"question\"]), \"question\": lambda x: x[\"question\"]}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "print(rag.invoke({\"question\": \"Czym jest RAG i do czego służy FAISS?\"}))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
