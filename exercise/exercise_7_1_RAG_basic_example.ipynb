{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52db5faf",
   "metadata": {},
   "source": [
    "# excercise_7_1_RAG_basic_example\n",
    "\n",
    "**Temat:** RAG — chunking → wektory → retrieval → szkic promptu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb77cbd",
   "metadata": {},
   "source": [
    "## Zadanie 1 — Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_len: int = 80) -> list[str]:\n",
    "    words, chunks, cur = text.split(), [], \"\"\n",
    "    for w in words:\n",
    "        if len(cur) + len(w) + 1 <= max_len:\n",
    "            cur = (cur + \" \" + w).strip()\n",
    "        else:\n",
    "            chunks.append(cur)\n",
    "            cur = w\n",
    "    if cur: chunks.append(cur)\n",
    "    return chunks\n",
    "txt = \"LangChain ułatwia budowę aplikacji opartych o LLM. W RAG łączymy retrieval i generację.\"\n",
    "print(chunk_text(txt, 40))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059112a",
   "metadata": {},
   "source": [
    "## Zadanie 2 — Proste featury i najbliższe sąsiedztwo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def featurize(s: str) -> np.ndarray:\n",
    "    words = s.split()\n",
    "    return np.array([len(s), len(set(words))], dtype=float)  # TODO: prosta cecha\n",
    "def build_index(chunks): return [featurize(c) for c in chunks]\n",
    "def nearest(query, chunks, index, top_k=2):\n",
    "    qv = featurize(query)\n",
    "    dists = [np.linalg.norm(qv - v) for v in index]\n",
    "    order = np.argsort(dists)[:top_k]\n",
    "    return [(chunks[i], dists[i]) for i in order]\n",
    "doc_chunks = chunk_text(txt, 40)\n",
    "index = build_index(doc_chunks)\n",
    "print(nearest(\"retrieval generacja\", doc_chunks, index, top_k=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2bc944",
   "metadata": {},
   "source": [
    "## Zadanie 3 — Retrieval → szkic promptu do LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eca736",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = [c for c, _ in nearest(\"Jak łączyć retrieval i generację?\", doc_chunks, index, top_k=2)]\n",
    "prompt = f\"Kontekst: {top}\\n\\nPytanie: Jak łączyć retrieval i generację?\"\n",
    "print(prompt[:200], \"...\")\n",
    "assert \"Kontekst:\" in prompt and \"Pytanie:\" in prompt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}