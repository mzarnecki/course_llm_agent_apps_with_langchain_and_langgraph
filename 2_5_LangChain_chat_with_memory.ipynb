{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Chat z pamięcią",
   "id": "fa55855baa5d0aa5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Instalacja bibliotek",
   "id": "6d16ab59db57af29"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-06T06:23:02.294895Z",
     "start_time": "2025-10-06T06:23:00.575748Z"
    }
   },
   "source": "!pip install -q langchain-openai python-dotenv",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Utworzenie modelu",
   "id": "d8fdb3af72d3a768"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T06:23:02.322119Z",
     "start_time": "2025-10-06T06:23:02.309430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "eb55511e53f3a772",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### History\n",
    "History – przechowuje i wstrzykuje do promptu całą historię rozmowy wiadomość po wiadomości."
   ],
   "id": "155809f08030346c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T06:23:26.874094Z",
     "start_time": "2025-10-06T06:23:25.014612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "# message history\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"Buenos dias!\")\n",
    "history.add_ai_message(\"hello!\")\n",
    "history.add_user_message(\"Whats your name?\")\n",
    "history.add_ai_message(\"My name is GIGACHAT\")\n",
    "\n",
    "# history.messages"
   ],
   "id": "8634db480b86ed6f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T06:23:42.612190Z",
     "start_time": "2025-10-06T06:23:42.607422Z"
    }
   },
   "cell_type": "code",
   "source": "history.messages",
   "id": "1dc59fc6574e8ed9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buenos dias!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hello!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Whats your name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='My name is GIGACHAT', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Memory",
   "id": "b66acacd555b1e9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T06:27:51.174130Z",
     "start_time": "2025-10-06T06:27:41.137783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "#typy wiadomości: user, assistant, system, function, tool\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Prowadzisz przyjazną rozmowę i pamiętasz kontekst. \"\n",
    "               \"Zawsze odpowiadaj po polsku.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "store = {}\n",
    "def get_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "sid = \"demo-session-123\"\n",
    "\n",
    "resp1 = chain_with_memory.invoke(\n",
    "    {\"input\": \"Cześć! Mam na imię Michał.\"},\n",
    "    config={\"configurable\": {\"session_id\": sid}}\n",
    ")\n",
    "print(resp1.content)\n",
    "\n",
    "resp2 = chain_with_memory.invoke(\n",
    "    {\"input\": \"Jak mam na imię?\"},\n",
    "    config={\"configurable\": {\"session_id\": sid}}\n",
    ")\n",
    "print(resp2.content)\n"
   ],
   "id": "ec4e0329412c0a52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cześć, Michał! Miło mi cię poznać. Jak mija twój dzień?\n",
      "Masz na imię Michał. Jakie masz plany na dzisiaj?\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary\n",
    "Summary (memory) – zamiast pełnej historii przekazuje do modelu skondensowane podsumowanie wcześniejszych rozmów, co oszczędza tokeny i ułatwia skalowanie długich dialogów."
   ],
   "id": "5b001aa00eca05a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T06:31:23.142229Z",
     "start_time": "2025-10-06T06:31:15.259348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "summarizer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Summarize the following conversation briefly:\"),\n",
    "    (\"human\", \"{conversation}\")\n",
    "])\n",
    "summarizer = summarizer_prompt | llm | StrOutputParser()\n",
    "\n",
    "conversation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Here is the summary of prior conversation:\\n{summary}\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "conversation_chain = conversation_prompt | llm | StrOutputParser()\n",
    "\n",
    "store = {}\n",
    "summaries = {}\n",
    "\n",
    "def get_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "def get_summary(session_id: str) -> str:\n",
    "    if session_id not in summaries:\n",
    "        summaries[session_id] = \"No previous conversation.\"\n",
    "    return summaries[session_id]\n",
    "\n",
    "def update_summary(session_id: str, threshold: int = 4):\n",
    "    \"\"\"Update summary and clear old messages when threshold is reached\"\"\"\n",
    "    history = get_history(session_id)\n",
    "\n",
    "    if len(history.messages) >= threshold:\n",
    "        current_summary = summaries.get(session_id, \"\")\n",
    "\n",
    "        if current_summary and current_summary != \"No previous conversation.\":\n",
    "            conversation_text = f\"Previous summary: {current_summary}\\n\\nRecent messages: {history.messages}\"\n",
    "        else:\n",
    "            conversation_text = str(history.messages)\n",
    "\n",
    "        new_summary = summarizer.invoke({\"conversation\": conversation_text})\n",
    "        summaries[session_id] = new_summary\n",
    "\n",
    "        history.clear()\n",
    "\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    conversation_chain,\n",
    "    get_session_history=get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "sid = \"demo-summary\"\n",
    "\n",
    "cfg = {\"configurable\": {\"session_id\": sid}}\n",
    "summary = get_summary(sid)\n",
    "response1 = chain_with_memory.invoke({\"input\": \"Cześć! Mam na imię Michał.\", \"summary\": summary}, cfg)\n",
    "print(response1)\n",
    "\n",
    "# Update summary periodically (e.g., after every 2-3 exchanges)\n",
    "update_summary(sid, threshold=4)\n",
    "\n",
    "summary = get_summary(sid)\n",
    "response2 = chain_with_memory.invoke({\"input\": \"Jak mam na imię?\", \"summary\": summary}, cfg)\n",
    "print(response2)\n",
    "\n",
    "update_summary(sid, threshold=4)\n"
   ],
   "id": "8aeae48dc35063df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cześć, Michał! Jak mogę Ci dzisiaj pomóc?\n",
      "Masz na imię Michał. Jakie masz pytania lub o czym chciałbyś porozmawiać?\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
