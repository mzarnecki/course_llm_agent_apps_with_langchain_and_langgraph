{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Chat z pamięcią",
   "id": "fa55855baa5d0aa5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Instalacja bibliotek",
   "id": "6d16ab59db57af29"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-16T06:54:41.592189Z",
     "start_time": "2025-09-16T06:54:40.150780Z"
    }
   },
   "source": "!pip install -q langchain-openai python-dotenv langchain-core tiktoken",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Utworzenie modelu",
   "id": "d8fdb3af72d3a768"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T06:54:47.441641Z",
     "start_time": "2025-09-16T06:54:47.436560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # możesz podmienić na inny wspierany model\n",
    "    temperature=0         # 0 = maksymalna przewidywalność, dobre do testów pamięci\n",
    ")\n",
    "\n",
    "print(\"Model gotowy.\")"
   ],
   "id": "eb55511e53f3a772",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gotowy.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### History\n",
    "History – przechowuje i wstrzykuje do promptu całą historię rozmowy wiadomość po wiadomości."
   ],
   "id": "155809f08030346c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T06:54:52.403744Z",
     "start_time": "2025-09-16T06:54:52.397850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "# message history\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"Buenos dias!\")\n",
    "history.add_ai_message(\"hello!\")\n",
    "history.add_user_message(\"Whats your name?\")\n",
    "history.add_ai_message(\"My name is GIGACHAT\")\n",
    "history.messages"
   ],
   "id": "8634db480b86ed6f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buenos dias!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hello!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Whats your name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='My name is GIGACHAT', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T06:59:17.382598Z",
     "start_time": "2025-09-16T06:59:15.333823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Prowadzisz przyjazną rozmowę i pamiętasz kontekst. \"\n",
    "               \"Zawsze odpowiadaj po polsku.\"),\n",
    "    MessagesPlaceholder(\"history\"),          # <-- kluczowe!\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm  # bez StrOutputParser, zapisze AIMessage do historii\n",
    "\n",
    "store = {}\n",
    "def get_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    "    # output_messages_key domyślne działa; zostawiamy\n",
    ")\n",
    "\n",
    "sid = \"demo-session-1\"\n",
    "\n",
    "resp1 = chain_with_memory.invoke(\n",
    "    {\"input\": \"Cześć! Mam na imię Michał.\"},\n",
    "    config={\"configurable\": {\"session_id\": sid}}\n",
    ")\n",
    "print(resp1.content)\n",
    "\n",
    "resp2 = chain_with_memory.invoke(\n",
    "    {\"input\": \"Jak mam na imię?\"},\n",
    "    config={\"configurable\": {\"session_id\": sid}}\n",
    ")\n",
    "print(resp2.content)\n"
   ],
   "id": "ec4e0329412c0a52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cześć, Michał! Miło mi cię poznać. Jak mija twój dzień?\n",
      "Masz na imię Michał. Jakie masz plany na dzisiaj?\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Memory\n",
    "Memory – zarządza kontekstem rozmowy w bardziej zaawansowany sposób, np. buforuje tylko ostatnie N wiadomości albo wyodrębnia istotne fakty."
   ],
   "id": "3b1266e29eb05138"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T06:59:56.906642Z",
     "start_time": "2025-09-16T06:59:55.483225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# --- Model ---\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# --- Prompt with history slot ---\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in the same language as the user and greet him/her using his/her name.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    "    output_messages_key=\"output\"\n",
    ")\n",
    "\n",
    "# --- Usage ---\n",
    "cfg = {\"configurable\": {\"session_id\": \"test-session\"}}\n",
    "history = get_session_history(\"test-session\")\n",
    "\n",
    "# preload conversation history (like ConversationBufferMemory did)\n",
    "history.add_user_message(\"Buenos dias!\")\n",
    "history.add_ai_message(\"Hello!\")\n",
    "history.add_user_message(\"My name is Michał. What's your name?\")\n",
    "history.add_ai_message(\"My name is GIGACHAT\")\n",
    "\n",
    "# now query\n",
    "print(conversation.invoke({\"input\": \"Buenos Dias!\"}, cfg))\n"
   ],
   "id": "e571d8a28e1ec8c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Buenos días, Michał! ¿Cómo estás hoy?\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary\n",
    "Summary (memory) – zamiast pełnej historii przekazuje do modelu skondensowane podsumowanie wcześniejszych rozmów, co oszczędza tokeny i ułatwia skalowanie długich dialogów."
   ],
   "id": "5b001aa00eca05a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T07:04:33.566237Z",
     "start_time": "2025-09-16T07:04:29.465454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# --- Summarizer chain ---\n",
    "summarizer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Summarize the following conversation briefly:\"),\n",
    "    (\"human\", \"{conversation}\")\n",
    "])\n",
    "summarizer = summarizer_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --- Conversation prompt (with summary injected) ---\n",
    "conversation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Here is the summary of prior conversation:\\n{summary}\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "conversation_chain = conversation_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --- Storage for sessions ---\n",
    "store = {}\n",
    "\n",
    "def get_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# --- Wrapper with history ---\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    conversation_chain,\n",
    "    get_session_history=get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    "    output_messages_key=\"output\"\n",
    ")\n",
    "\n",
    "# --- Function to get summary before invoking ---\n",
    "def get_summary(session_id: str) -> str:\n",
    "    history = get_history(session_id).messages\n",
    "    if not history:\n",
    "        return \"No previous conversation.\"\n",
    "    return summarizer.invoke({\"conversation\": history})\n",
    "\n",
    "# --- Usage ---\n",
    "sid = \"demo-summary\"\n",
    "\n",
    "# First round\n",
    "cfg = {\"configurable\": {\"session_id\": sid}}\n",
    "summary = get_summary(sid)\n",
    "print(chain_with_memory.invoke({\"input\": \"Cześć! Mam na imię Michał.\", \"summary\": summary}, cfg))\n",
    "\n",
    "# Next round, now with updated summary\n",
    "summary = get_summary(sid)\n",
    "print(chain_with_memory.invoke({\"input\": \"Jak mam na imię?\", \"summary\": summary}, cfg))\n"
   ],
   "id": "8aeae48dc35063df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cześć, Michał! Jak mogę Ci dzisiaj pomóc?\n",
      "Masz na imię Michał. Jakie masz pytania lub w czym mogę Ci pomóc?\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
