{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2. Komponenty biblioteki LangChain",
   "id": "be304df0c16cc932"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Instalacja",
   "id": "a4b1b26363bfed05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install -q python-dotenv langchain langchain-openai langchain-community langchain-text-splitters faiss-cpu"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Konfiguracja .env i modelu",
   "id": "20734714333be78c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Jesteś pomocnym asystentem. Odpowiadaj zwięźle.\"),\n",
    "    (\"user\", \"Streszcz w 1 zdaniu: {tekst}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()  # LCEL: prompt → model → parser\n",
    "wynik = chain.invoke({\"tekst\": \"LangChain ułatwia budowę aplikacji LLM, dostarczając klocki do promptów, pamięci, narzędzi i RAG.\"})\n",
    "print(wynik)\n"
   ],
   "id": "fd2b0cdbcb177adb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompt template",
   "id": "9f0f36cc0a222968"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "template = \"\"\"\n",
    "Specify tags and the main topic of the text.\n",
    "tags: What are best tags describing text. Give maximum 5 tags separated by comma.\n",
    "topic: What is the topic of text. Use maximum couple of words\n",
    "\n",
    "Format response as JSON as below:\n",
    "'tags': ['sometag', 'othertag', 'anothertag', 'tag4', 'tag5']\n",
    "'subject': 'Some subject of text'\n",
    "\n",
    "text: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=template)\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "chain.predict(input=\"They picked a way among the trees, and their ponies plodded along, carefully avoiding the many writhing and interlacing roots.  There was no undergrowth.  The ground was rising steadily, and as they went forward it seemed that the trees became taller, darker, and thicker. There was no sound, except an occasional drip of moisture falling through the still leaves.  For the moment there was no whispering or movement among the branches; but they all got an uncomfortable feeling that they were being watched with disapproval, deepening to dislike and even enmity.  The feeling steadily grew, until they found themselves looking up quickly, or glancing back over their shoulders, as if they expected a sudden blow.\")"
   ],
   "id": "58d640276960a0c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ResponseSchema i OutputParser",
   "id": "c7e817465b616462"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "tags_schema = ResponseSchema(\n",
    "    name=\"tags\",\n",
    "    description=\" What are best tags describing text. Give maximum 5 tags separated by comma.\",\n",
    ")\n",
    "topic_schema = ResponseSchema(\n",
    "    name=\"topic\",\n",
    "    description=\"What is the topic of text. Use maximum couple of words.\"\n",
    ")\n",
    "\n",
    "response_schemas = [tags_schema, topic_schema]\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "template = \"\"\"\n",
    "Specify tags and the main topic of the text.\n",
    "tags: What are best tags describing text. Give maximum 5 tags separated by comma.\n",
    "topic: What is the topic of text. Use maximum couple of words\n",
    "\n",
    "Format response as JSON as below:\n",
    "'tags': ['sometag', 'othertag', 'anothertag', 'tag4', 'tag5']\n",
    "'subject': 'Some subject of text'\n",
    "\n",
    "text: {input}\n",
    "\n",
    "{instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "instructions = parser.get_format_instructions()\n",
    "\n",
    "messages = prompt.format_messages(\n",
    "    input=\"They picked a way among the trees, and their ponies plodded along, carefully avoiding the many writhing and interlacing roots.  There was no undergrowth.  The ground was rising steadily, and as they went forward it seemed that the trees became taller, darker, and thicker. There was no sound, except an occasional drip of moisture falling through the still leaves.  For the moment there was no whispering or movement among the branches; but they all got an uncomfortable feeling that they were being watched with disapproval, deepening to dislike and even enmity.  The feeling steadily grew, until they found themselves looking up quickly, or glancing back over their shoulders, as if they expected a sudden blow.\",\n",
    "    instructions=instructions,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "response = chat(messages)\n",
    "print(response)\n",
    "output_dict = parser.parse(response.content)\n",
    "print(output_dict)"
   ],
   "id": "765ef2a9b758f243"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prompty, Parsowanie, LCEL / Chains\n",
    "PromptTemplate + LLM + StrOutputParser (LCEL)"
   ],
   "id": "df7a28a0072f3eae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Jesteś pomocnym asystentem. Odpowiadaj zwięźle.\"),\n",
    "    (\"user\", \"Streszcz w 1 zdaniu: {tekst}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()  # LCEL: prompt → model → parser\n",
    "wynik = chain.invoke({\"tekst\": \"LangChain ułatwia budowę aplikacji LLM, dostarczając klocki do promptów, pamięci, narzędzi i RAG.\"})\n",
    "print(wynik)\n"
   ],
   "id": "1e9ee8a24bb448f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tools (narzędzia, które może wywołać model)\n",
    "prosty tool (kalkulator) + agent ReAct"
   ],
   "id": "347bfefc09014c43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Zwraca sumę a+b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Zwraca iloczyn a*b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "from langchain import agents\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "react_prompt = PromptTemplate.from_template(\n",
    "\"\"\"Jesteś pomocnym asystentem. Masz dostęp do narzędzi.\n",
    "Używaj ich tylko gdy potrzebne. Odpowiadaj po polsku.\n",
    "\n",
    "Pytanie: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    ")\n",
    "\n",
    "agent = create_react_agent(llm, tools, react_prompt)\n",
    "executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "response = executor.invoke({\"input\": \"Policz (12 + 7) * 3 i podaj wynik.\"})\n",
    "print(\"\\nWynik końcowy:\", response[\"output\"])\n"
   ],
   "id": "cbe2d51c2fa14fb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Memory (historia rozmowy / stan)\n",
    "RunnableWithMessageHistory (pamięć czatu)"
   ],
   "id": "c928b1583cceade5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Prowadzisz przyjazną rozmowę i pamiętasz kontekst.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chat_chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "store = {}  # prosta “baza” historii po session_id\n",
    "\n",
    "def get_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chat_chain,\n",
    "    get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "sid = \"demo-session-1\"\n",
    "print(chain_with_memory.invoke({\"input\": \"Cześć! Mam na imię Michał.\"},\n",
    "                               config={\"configurable\": {\"session_id\": sid}}))\n",
    "print(chain_with_memory.invoke({\"input\": \"Jak mam na imię?\"},\n",
    "                               config={\"configurable\": {\"session_id\": sid}}))\n"
   ],
   "id": "9762f0fd06e70f4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### History",
   "id": "d10c6b73f72d23c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ],
   "id": "c3fc5f2f1e9d74b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# message history\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"Buenos dias!\")\n",
    "history.add_ai_message(\"hello!\")\n",
    "history.add_user_message(\"Whats your name?\")\n",
    "history.add_ai_message(\"My name is GIGACHAT\")\n",
    "history.messages"
   ],
   "id": "71ecd32594bf3dda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Memory",
   "id": "bf1a30db19a6c9ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# memory\n",
    "# History keeps all messages between the user and AI intact. History is what the user sees in the UI.\n",
    "# It represents what was actually said. Memory keeps some information, which is presented to the LLM to make it behave as if it \"remembers\" the conversation.\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"Buenos dias!\")\n",
    "memory.chat_memory.add_ai_message(\"Hello!\")\n",
    "memory.chat_memory.add_user_message(\"Whats your name?\")\n",
    "memory.chat_memory.add_ai_message(\"My name is GIGACHAT\")\n",
    "memory.load_memory_variables({})"
   ],
   "id": "51e3819f14eb5b3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=True, memory=memory\n",
    ")\n",
    "conversation.predict(input=\"Buenos Dias!\") # will it response in different language?"
   ],
   "id": "d255837802009235"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Summary",
   "id": "91ce1fe6c1c562a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install tiktoken",
   "id": "f68f37f98302d2cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "previous_output_review = \"Rdr2 is an experience, as it's more than just a video game. It's like being in one long Oscar winning movie. Script, acting, storyline are immense, it's quite unbelievable really. You are living as a cowboy day in day out eating, bathing, sleeping, everything in a huge openworld.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=1-0)\n",
    "memory.save_context(\n",
    "    {\"input\": f\"Could you analyze review for me {previous_output_review}?\"},\n",
    "    {\"output\": \"Sure, no problem\"},\n",
    ")"
   ],
   "id": "506466fbdd6c0e60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)",
   "id": "921a0c2fb84b36c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "conversation.predict(input=\"Thank you\")",
   "id": "3c53b0478c8f324c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
