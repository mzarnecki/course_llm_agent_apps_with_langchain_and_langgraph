{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2. Komponenty biblioteki LangChain",
   "id": "be304df0c16cc932"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### instalacja",
   "id": "a4b1b26363bfed05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install -q python-dotenv langchain langchain-openai langchain-community langchain-text-splitters faiss-cpu"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### konfiguracja .env i modelu",
   "id": "20734714333be78c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Jesteś pomocnym asystentem. Odpowiadaj zwięźle.\"),\n",
    "    (\"user\", \"Streszcz w 1 zdaniu: {tekst}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()  # LCEL: prompt → model → parser\n",
    "wynik = chain.invoke({\"tekst\": \"LangChain ułatwia budowę aplikacji LLM, dostarczając klocki do promptów, pamięci, narzędzi i RAG.\"})\n",
    "print(wynik)\n"
   ],
   "id": "fd2b0cdbcb177adb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prompty, Parsowanie, LCEL / Chains\n",
    "PromptTemplate + LLM + StrOutputParser (LCEL)"
   ],
   "id": "df7a28a0072f3eae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Jesteś pomocnym asystentem. Odpowiadaj zwięźle.\"),\n",
    "    (\"user\", \"Streszcz w 1 zdaniu: {tekst}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()  # LCEL: prompt → model → parser\n",
    "wynik = chain.invoke({\"tekst\": \"LangChain ułatwia budowę aplikacji LLM, dostarczając klocki do promptów, pamięci, narzędzi i RAG.\"})\n",
    "print(wynik)\n"
   ],
   "id": "1e9ee8a24bb448f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tools (narzędzia, które może wywołać model)\n",
    "prosty tool (kalkulator) + agent ReAct"
   ],
   "id": "347bfefc09014c43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Zwraca sumę a+b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Zwraca iloczyn a*b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "from langchain import agents\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "react_prompt = PromptTemplate.from_template(\n",
    "\"\"\"Jesteś pomocnym asystentem. Masz dostęp do narzędzi.\n",
    "Używaj ich tylko gdy potrzebne. Odpowiadaj po polsku.\n",
    "\n",
    "Pytanie: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    ")\n",
    "\n",
    "agent = create_react_agent(llm, tools, react_prompt)\n",
    "executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "response = executor.invoke({\"input\": \"Policz (12 + 7) * 3 i podaj wynik.\"})\n",
    "print(\"\\nWynik końcowy:\", response[\"output\"])\n"
   ],
   "id": "cbe2d51c2fa14fb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Memory (historia rozmowy / stan)\n",
    "RunnableWithMessageHistory (pamięć czatu)"
   ],
   "id": "c928b1583cceade5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Prowadzisz przyjazną rozmowę i pamiętasz kontekst.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chat_chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "store = {}  # prosta “baza” historii po session_id\n",
    "\n",
    "def get_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chat_chain,\n",
    "    get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "sid = \"demo-session-1\"\n",
    "print(chain_with_memory.invoke({\"input\": \"Cześć! Mam na imię Michał.\"},\n",
    "                               config={\"configurable\": {\"session_id\": sid}}))\n",
    "print(chain_with_memory.invoke({\"input\": \"Jak mam na imię?\"},\n",
    "                               config={\"configurable\": {\"session_id\": sid}}))\n"
   ],
   "id": "9762f0fd06e70f4d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
