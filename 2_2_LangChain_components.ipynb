{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2. Komponenty biblioteki LangChain",
   "id": "be304df0c16cc932"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### instalacja",
   "id": "a4b1b26363bfed05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install -q python-dotenv langchain langchain-openai langchain-community langchain-text-splitters faiss-cpu"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### konfiguracja .env i modelu",
   "id": "20734714333be78c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Jesteś pomocnym asystentem. Odpowiadaj zwięźle.\"),\n",
    "    (\"user\", \"Streszcz w 1 zdaniu: {tekst}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()  # LCEL: prompt → model → parser\n",
    "wynik = chain.invoke({\"tekst\": \"LangChain ułatwia budowę aplikacji LLM, dostarczając klocki do promptów, pamięci, narzędzi i RAG.\"})\n",
    "print(wynik)\n"
   ],
   "id": "fd2b0cdbcb177adb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prompty, Parsowanie, LCEL / Chains\n",
    "PromptTemplate + LLM + StrOutputParser (LCEL)"
   ],
   "id": "df7a28a0072f3eae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Jesteś pomocnym asystentem. Odpowiadaj zwięźle.\"),\n",
    "    (\"user\", \"Streszcz w 1 zdaniu: {tekst}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()  # LCEL: prompt → model → parser\n",
    "wynik = chain.invoke({\"tekst\": \"LangChain ułatwia budowę aplikacji LLM, dostarczając klocki do promptów, pamięci, narzędzi i RAG.\"})\n",
    "print(wynik)\n"
   ],
   "id": "1e9ee8a24bb448f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tools (narzędzia, które może wywołać model)\n",
    "prosty tool (kalkulator) + agent ReAct"
   ],
   "id": "347bfefc09014c43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Zwraca sumę a+b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Zwraca iloczyn a*b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "from langchain import agents\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "react_prompt = PromptTemplate.from_template(\n",
    "\"\"\"Jesteś pomocnym asystentem. Masz dostęp do narzędzi.\n",
    "Używaj ich tylko gdy potrzebne. Odpowiadaj po polsku.\n",
    "\n",
    "Pytanie: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    ")\n",
    "\n",
    "agent = create_react_agent(llm, tools, react_prompt)\n",
    "executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "odp = executor.invoke({\"input\": \"Policz (12 + 7) * 3 i podaj wynik.\"})\n",
    "print(\"\\nWynik końcowy:\", odp[\"output\"])\n"
   ],
   "id": "cbe2d51c2fa14fb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### budowa VectorStore (FAISS) i Retrievera",
   "id": "a96230abca2f2300"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "emb = OpenAIEmbeddings()  # wymaga OPENAI_API_KEY\n",
    "vectorstore = FAISS.from_texts(splitted, embedding=emb)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "query = \"Po co używa się retrievera?\"\n",
    "context = retriever.get_relevant_documents(query)\n",
    "print(\"Znaleziony kontekst:\")\n",
    "for i, c in enumerate(context, 1):\n",
    "    print(f\"{i}.\", c.page_content)\n"
   ],
   "id": "e678e9c2153ffea3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### prosty łańcuch RAG (prompt + kontekst + LLM)",
   "id": "c6afc2b088f568ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Udziel precyzyjnej odpowiedzi wyłącznie na podstawie KONTEKSTU. Jeśli brak danych — powiedz, że nie wiesz.\"),\n",
    "    (\"system\", \"KONTEKST:\\n{context}\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(rag_chain.invoke(\"Czym jest FAISS i do czego służy?\"))\n"
   ],
   "id": "52c20feeabc9921b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Memory (historia rozmowy / stan)\n",
    "RunnableWithMessageHistory (pamięć czatu)"
   ],
   "id": "c928b1583cceade5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Prowadzisz przyjazną rozmowę i pamiętasz kontekst.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chat_chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "store = {}  # prosta “baza” historii po session_id\n",
    "\n",
    "def get_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chat_chain,\n",
    "    get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "sid = \"demo-session-1\"\n",
    "print(chain_with_memory.invoke({\"input\": \"Cześć! Mam na imię Michał.\"},\n",
    "                               config={\"configurable\": {\"session_id\": sid}}))\n",
    "print(chain_with_memory.invoke({\"input\": \"Jak mam na imię?\"},\n",
    "                               config={\"configurable\": {\"session_id\": sid}}))\n"
   ],
   "id": "9762f0fd06e70f4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LangSmith (tracing) — opcjonalnie\n",
    "włącz śledzenie (jeśli masz konto)"
   ],
   "id": "beb75e24a66e13c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Opcjonalnie (wymaga konta):\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = \"<TWÓJ_KLUCZ>\"\n",
    "# os.environ[\"LANGSMITH_PROJECT\"] = \"kurs-demo\"\n",
    "print(\"LangSmith: ustaw zmienne środowiskowe, aby włączyć tracing.\")\n"
   ],
   "id": "423067f27d1e01ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LangGraph (mini-graf stanów)\n",
    "minimalny graf: plan → odpowiedź"
   ],
   "id": "e6579bd66d15ca3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Minimalny przykład z LangGraph: dwa węzły (plan, odpowiedź)\n",
    "!pip -q install langgraph\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    plan: str\n",
    "    answer: str\n",
    "\n",
    "def plan_node(state: State):\n",
    "    q = state[\"question\"]\n",
    "    return {\"plan\": f\"1) Zrozumieć pytanie: {q}\\n2) Odpowiedzieć krótko.\"}\n",
    "\n",
    "def answer_node(state: State):\n",
    "    p = state[\"plan\"]\n",
    "    msg = f\"Wykonuję plan:\\n{p}\\n\\nOdpowiedź: To prosty przykład grafu stanów w LangGraph.\"\n",
    "    return {\"answer\": msg}\n",
    "\n",
    "g = StateGraph(State)\n",
    "g.add_node(\"plan\", plan_node)\n",
    "g.add_node(\"answer\", answer_node)\n",
    "g.set_entry_point(\"plan\")\n",
    "g.add_edge(\"plan\", \"answer\")\n",
    "g.add_edge(\"answer\", END)\n",
    "app = g.compile()\n",
    "\n",
    "out = app.invoke({\"question\": \"Co to jest LangGraph?\"})\n",
    "print(out[\"answer\"])\n"
   ],
   "id": "fef6bbf0a9f4382c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
