{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LangChain chains",
   "id": "3548fe4a952ca65f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install -q langchain langchain-openai python-dotenv"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T14:47:23.360311Z",
     "start_time": "2025-08-31T14:47:22.282778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(\"Model gotowy.\")\n"
   ],
   "id": "1eea8d7732ecebee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gotowy.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prosty chain: prompt → model → wynik",
   "id": "cd970036ab362bbd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T14:47:46.505075Z",
     "start_time": "2025-08-31T14:47:44.270719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Definiujemy prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Jesteś pomocnym asystentem.\"),\n",
    "    (\"user\", \"Streść w jednym zdaniu: {tekst}\")\n",
    "])\n",
    "\n",
    "# Tworzymy chain: prompt → llm → parser\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Test\n",
    "wynik = chain.invoke({\"tekst\": \"LangChain to biblioteka do pracy z dużymi modelami językowymi.\"})\n",
    "print(wynik)\n"
   ],
   "id": "a45f935ced8f41cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain to biblioteka umożliwiająca efektywne wykorzystanie dużych modeli językowych w różnych aplikacjach.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sequential chain: dwa modele w sekwencji",
   "id": "2cac72131d772ec2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T14:48:10.760761Z",
     "start_time": "2025-08-31T14:48:08.437779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 1) Chain: streszczenie (wejście: {tekst} → wyjście: str)\n",
    "summary_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Streszcz poniższy tekst w 1–2 zdaniach po polsku.\"),\n",
    "        (\"user\", \"{tekst}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 2) Adapter: zamień str → {\"tekst\": str}, żeby podać do następnego promptu\n",
    "to_dict = RunnableLambda(lambda s: {\"tekst\": s})\n",
    "\n",
    "# 3) Chain: tłumaczenie streszczenia na francuski (wejście: {tekst} → wyjście: str)\n",
    "translate_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Przetłumacz tekst na język francuski.\"),\n",
    "        (\"user\", \"{tekst}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 4) Prawdziwy sequential chain: summary → (wrap) → translate\n",
    "sequential_chain = summary_chain | to_dict | translate_chain\n",
    "\n",
    "# Test — jedno wywołanie całego łańcucha\n",
    "input_text = \"LangChain umożliwia tworzenie aplikacji AI poprzez łączenie modeli, promptów i narzędzi w spójne pipeline’y.\"\n",
    "final_translation = sequential_chain.invoke({\"tekst\": input_text})\n",
    "\n",
    "print(\"Tłumaczenie francuskie:\\n\", final_translation)\n",
    "\n"
   ],
   "id": "92240f604a750d3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streszczenie: LangChain pozwala na budowanie aplikacji AI poprzez integrację modeli, promptów i narzędzi w zorganizowane procesy robocze.\n",
      "Tłumaczenie: LangChain permet de construire des applications d'IA en intégrant des modèles, des invites et des outils dans des processus de travail organisés.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Branching chain: jedna odpowiedź, dwa przetworzenia",
   "id": "256cc9e98cf0ddff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Prompt do streszczenia\n",
    "prompt_summary = ChatPromptTemplate.from_template(\"Streszcz: {tekst}\")\n",
    "\n",
    "# Prompt do sentymentu\n",
    "prompt_sentiment = ChatPromptTemplate.from_template(\"Określ ton wypowiedzi: {tekst}\")\n",
    "\n",
    "# Dwa równoległe przetworzenia\n",
    "branch_chain = RunnableParallel(\n",
    "    summary=(prompt_summary | llm | StrOutputParser()),\n",
    "    sentiment=(prompt_sentiment | llm | StrOutputParser())\n",
    ")\n",
    "\n",
    "text = \"Jestem bardzo zadowolony z tego kursu, nauczyłem się dużo o LangChain!\"\n",
    "result = branch_chain.invoke({\"tekst\": text})\n",
    "\n",
    "print(result)\n"
   ],
   "id": "783d3e92e71467ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prosty RAG chain",
   "id": "5be3f300874a387f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "prosty chain: prompt → model → wynik\n",
    "# Dokumenty\n",
    "docs = [\n",
    "    \"LangChain to framework do pracy z dużymi modelami językowymi.\",\n",
    "    \"Chains w LangChain to przepływy danych pomiędzy promptami, modelami i parserami.\",\n",
    "    \"Retriever pozwala wyszukiwać informacje w bazie wektorowej.\"\n",
    "]\n",
    "\n",
    "# Split i embeddings\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n",
    "splitted = splitter.split_text(\" \".join(docs))\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_texts(splitted, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Prompt RAG\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Odpowiedz tylko na podstawie KONTEKSTU:\\n{context}\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(rag_chain.invoke(\"Czym są chains w LangChain?\"))\n"
   ],
   "id": "3d14a63d35ddbed8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
