{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.4. Przykład - chat z pamięcią",
   "id": "fa55855baa5d0aa5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instalujemy tylko to, czego potrzebujemy do chatu z pamięcią.\n",
    "!pip install -q langchain-openai python-dotenv langchain-core\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1) Wczytujemy zmienne środowiskowe z .env (w tym OPENAI_API_KEY)\n",
    "load_dotenv()\n",
    "\n",
    "# 2) Inicjalizujemy model czatowy od OpenAI przez adapter langchain-openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # możesz podmienić na inny wspierany model\n",
    "    temperature=0         # 0 = maksymalna przewidywalność, dobre do testów pamięci\n",
    ")\n",
    "\n",
    "print(\"Model gotowy.\")\n"
   ],
   "id": "eb55511e53f3a772"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Użyjemy ChatPromptTemplate do zdefiniowania \"roli\" i wejścia użytkownika.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# SYSTEM: instrukcje dla modelu (styl, rola)\n",
    "# USER:   wiadomość użytkownika (podamy ją później jako {input})\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Prowadzisz przyjazną rozmowę po polsku i pamiętasz kontekst.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# LCEL (LangChain Expression Language): prompt -> model -> parser\n",
    "# Na razie to \"goły\" łańcuch bez pamięci.\n",
    "base_chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"Łańcuch bazowy gotowy (bez pamięci).\")\n"
   ],
   "id": "8634db480b86ed6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Do pamięci użyjemy wbudowanej klasy InMemoryChatMessageHistory\n",
    "# oraz wrappera RunnableWithMessageHistory, który \"wstrzykuje\"\n",
    "# historię do promptu na podstawie session_id.\n",
    "\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# \"store\" będzie prostą mapą: session_id -> historia wiadomości\n",
    "store = {}\n",
    "\n",
    "def get_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    \"\"\"Zwraca (lub tworzy) obiekt historii dla danej sesji.\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Opakowujemy nasz bazowy łańcuch w \"łańcuch z pamięcią\".\n",
    "# - input_messages_key: nazwa pola z nowym wejściem użytkownika\n",
    "# - history_messages_key: nazwa pola, pod którą wstrzykiwana jest historia\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_history,                     # funkcja pobierająca historię po session_id\n",
    "    input_messages_key=\"input\",      # ta sama nazwa co w prompt {input}\n",
    "    history_messages_key=\"history\",  # LangChain wstrzyknie historię do promptu\n",
    ")\n",
    "\n",
    "print(\"Łańcuch z pamięcią gotowy.\")\n"
   ],
   "id": "e571d8a28e1ec8c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Każda \"konwersacja\" ma swój session_id.\n",
    "# Dzięki temu możesz równolegle utrzymywać wiele niezależnych czatów.\n",
    "sid = \"demo-session-1\"\n",
    "\n",
    "# 1) Użytkownik przedstawia się\n",
    "resp1 = chain_with_memory.invoke(\n",
    "    {\"input\": \"Cześć! Nazywam się Michał i lubię programować w Pythonie.\"},\n",
    "    config={\"configurable\": {\"session_id\": sid}}\n",
    ")\n",
    "print(\"Bot:\", resp1)\n",
    "\n",
    "# 2) Pytanie zależne od kontekstu (model powinien \"pamiętać\" imię)\n",
    "resp2 = chain_with_memory.invoke(\n",
    "    {\"input\": \"Jak mam na imię i jaki język programowania lubię?\"},\n",
    "    config={\"configurable\": {\"session_id\": sid}}\n",
    ")\n",
    "print(\"Bot:\", resp2)\n",
    "\n",
    "# 3) Doprecyzowanie (pamięć + bieżący kontekst)\n",
    "resp3 = chain_with_memory.invoke(\n",
    "    {\"input\": \"Zgadza się. Dodaj, że prowadzę kurs o LangChain.\"},\n",
    "    config={\"configurable\": {\"session_id\": sid}}\n",
    ")\n",
    "print(\"Bot:\", resp3)\n",
    "e"
   ],
   "id": "8aeae48dc35063df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Nowe session_id = nowa, pusta pamięć\n",
    "sid2 = \"demo-session-2\"\n",
    "\n",
    "# Tu model nie powinien pamiętać, kim jest Michał z poprzedniej sesji.\n",
    "respA = chain_with_memory.invoke(\n",
    "    {\"input\": \"Kim jestem i co lubię programować?\"},\n",
    "    config={\"configurable\": {\"session_id\": sid2}}\n",
    ")\n",
    "print(\"Bot (sesja 2, bez kontekstu):\", respA)\n",
    "\n",
    "# Dodajemy informację w tej sesji...\n",
    "respB = chain_with_memory.invoke(\n",
    "    {\"input\": \"Mam na imię Ania i lubię Javę.\"},\n",
    "    config={\"configurable\": {\"session_id\": sid2}}\n",
    ")\n",
    "print(\"Bot:\", respB)\n",
    "\n",
    "# ...i sprawdzamy pamięć tylko w tej sesji:\n",
    "respC = chain_with_memory.invoke(\n",
    "    {\"input\": \"Jak mam na imię i jaki język lubię?\"},\n",
    "    config={\"configurable\": {\"session_id\": sid2}}\n",
    ")\n",
    "print(\"Bot:\", respC)\n"
   ],
   "id": "9f5e02e1dcc83bd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ta funkcja pozwala \"popykać\" w konsoli bez przełączania komórek.\n",
    "# Wywołuj: chat(\"demo-session-1\", \"Twoja wiadomość\")\n",
    "def chat(session_id: str, user_message: str):\n",
    "    \"\"\"Wyślij wiadomość do bota w ramach wskazanej sesji.\"\"\"\n",
    "    reply = chain_with_memory.invoke(\n",
    "        {\"input\": user_message},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    print(f\"[You @{session_id}]: {user_message}\")\n",
    "    print(f\"[Bot]: {reply}\")\n",
    "\n",
    "# Przykład:\n",
    "# chat(\"demo-session-1\", \"Przypomnij, co mówiłem o Pythonie.\")\n"
   ],
   "id": "c12e388532e0508b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "InMemoryChatMessageHistory trzyma listę wiadomości (user/assistant) dla danego session_id.\\\n",
    "RunnableWithMessageHistory:\\\n",
    "Przy każdym wywołaniu pobiera historię z get_history(session_id).\\\n",
    "Dokłada ją do promptu (klucz history), zanim model wygeneruje odpowiedź.\\\n",
    "Po wygenerowaniu odpowiedzi automatycznie dopisuje ją do historii.\\\n",
    "Dzięki temu kolejne wywołania w ramach tej samej sesji widzą wcześniejszy kontekst.\\\n",
    "Różne session_id ⇒ izolowane pamięci (oddzielne rozmowy).\\\n",
    "Chcesz wersję z podsumowującą pamięcią (automatyczne streszczenie starej historii, żeby oszczędzać tokeny), albo wariant, który serializuje historię do\\ pliku/Redis? Mogę dorzucić gotowe komórki.\\"
   ],
   "id": "a9f5fd22f6785ee4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
